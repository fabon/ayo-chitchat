{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of ParlAI Tutorial",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fabon/ayo-chitchat/blob/main/Copy_of_ParlAI_Tutorial.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zsb-Cvf6lnVX"
      },
      "source": [
        "<img src=\"https://parl.ai/docs/_static/img/parlai.png\" width=\"700\"/>\n",
        "\n",
        "**Author**: Stephen Roller ([GitHub](https://github.com/stephenroller), [Twitter](https://twitter.com/stephenroller))\n",
        "\n",
        "\n",
        "# Welcome to the ParlAI interactive tutorial\n",
        "\n",
        "In this tutorial we will:\n",
        "\n",
        "- Chat with a neural network model!\n",
        "- Show how to use common commands in ParlAI, like inspecting data and model outputs.\n",
        "- See where to find information about many options.\n",
        "- Show how to fine-tune a pretrained model on a specific task\n",
        "- Add our own datasets to ParlAI\n",
        "- And add our own models to ParlAI\n",
        "\n",
        "We won't be running any examples of using Amazon Mechanical Turk, or connecting to Chat services, but you can check out our [docs](https://parl.ai/docs/) for more information on these areas.\n",
        "\n",
        "**Note:** *Make sure you're running this session with a GPU attached.*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t_bFnOWslsj9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        },
        "outputId": "ed765e46-2eb6-422f-ea78-fb848824c4e2"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fri Oct 23 13:20:51 2020       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 455.23.05    Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   62C    P8    11W /  70W |      0MiB / 15079MiB |      0%      Default |\n",
            "|                               |                      |                 ERR! |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jMxd1KIRl9Xm"
      },
      "source": [
        "## Installing parlai\n",
        "\n",
        "We need to install ParlAI. Since we're in Google Colab, we can assume PyTorch and similar dependencies are installed already"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i93Mn_I7MOEO",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 748
        },
        "outputId": "8625a506-4033-4ecd-a0a0-a0643b2b2be0"
      },
      "source": [
        "!pip install -q parlai\n",
        "!pip install -q subword_nmt # extra requirement we need for this tutorial"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[K     |████████████████████████████████| 2.3MB 2.7MB/s \n",
            "\u001b[K     |████████████████████████████████| 61kB 8.4MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.1MB 20.5MB/s \n",
            "\u001b[K     |████████████████████████████████| 61kB 10.1MB/s \n",
            "\u001b[K     |████████████████████████████████| 51kB 9.3MB/s \n",
            "\u001b[K     |████████████████████████████████| 61kB 8.6MB/s \n",
            "\u001b[K     |████████████████████████████████| 5.7MB 30.6MB/s \n",
            "\u001b[K     |████████████████████████████████| 133kB 51.1MB/s \n",
            "\u001b[K     |████████████████████████████████| 655kB 52.8MB/s \n",
            "\u001b[K     |████████████████████████████████| 3.7MB 57.9MB/s \n",
            "\u001b[K     |████████████████████████████████| 204kB 55.2MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.5MB 52.7MB/s \n",
            "\u001b[K     |████████████████████████████████| 71kB 9.3MB/s \n",
            "\u001b[K     |████████████████████████████████| 460kB 53.4MB/s \n",
            "\u001b[K     |████████████████████████████████| 3.3MB 52.5MB/s \n",
            "\u001b[K     |████████████████████████████████| 61kB 9.2MB/s \n",
            "\u001b[K     |████████████████████████████████| 6.4MB 40.1MB/s \n",
            "\u001b[K     |████████████████████████████████| 276kB 53.9MB/s \n",
            "\u001b[K     |████████████████████████████████| 71kB 10.1MB/s \n",
            "\u001b[K     |████████████████████████████████| 235kB 49.8MB/s \n",
            "\u001b[K     |████████████████████████████████| 245kB 55.9MB/s \n",
            "\u001b[K     |████████████████████████████████| 552kB 53.4MB/s \n",
            "\u001b[K     |████████████████████████████████| 81kB 10.2MB/s \n",
            "\u001b[K     |████████████████████████████████| 71kB 9.6MB/s \n",
            "\u001b[K     |████████████████████████████████| 51kB 7.5MB/s \n",
            "\u001b[K     |████████████████████████████████| 61kB 8.6MB/s \n",
            "\u001b[K     |████████████████████████████████| 51kB 6.7MB/s \n",
            "\u001b[K     |████████████████████████████████| 92kB 10.9MB/s \n",
            "\u001b[K     |████████████████████████████████| 92kB 11.8MB/s \n",
            "\u001b[K     |████████████████████████████████| 102kB 13.2MB/s \n",
            "\u001b[K     |████████████████████████████████| 122kB 57.2MB/s \n",
            "\u001b[K     |████████████████████████████████| 92kB 11.8MB/s \n",
            "\u001b[?25h  Building wheel for websocket-server (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for emoji (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for regex (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for nltk (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for pyyaml (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for docformatter (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for untokenize (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: spacy 2.2.4 has requirement tqdm<5.0.0,>=4.38.0, but you'll have tqdm 4.36.1 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: google-colab 1.0.0 has requirement requests~=2.21.0, but you'll have requests 2.22.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: datascience 0.10.6 has requirement folium==0.2.1, but you'll have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: chainer 6.5.0 has requirement typing-extensions<=3.6.6, but you'll have typing-extensions 3.7.4.1 which is incompatible.\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KtVz5dCUmFkN"
      },
      "source": [
        "# Chatting with a model\n",
        "\n",
        "Let's start by chatting interactively with a model file from our model zoo! We'll pick our \"tutorial transformer generator\" model, which is a generative transformer trained on pushshift.io Reddit. You can take a look at the [model zoo](https://parl.ai/docs/zoo.html) for a more complete list."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nRJGRtMKmIWV",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "35be5dee-50ee-422e-e3f0-5c8e47e83a5b"
      },
      "source": [
        "# Import the Interactive script\n",
        "from parlai.scripts.interactive import Interactive\n",
        "\n",
        "# call it with particular args\n",
        "Interactive.main(\n",
        "    # the model_file is a filename path pointing to a particular model dump.\n",
        "    # Model files that begin with \"zoo:\" are special files distributed by the ParlAI team.\n",
        "    # They'll be automatically downloaded when you ask to use them.\n",
        "    model_file='zoo:tutorial_transformer_generator/model'\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\rDownloading tutorial_transformer_generator_v1.tar.gz: 0.00B [00:00, ?B/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[building data: /usr/local/lib/python3.6/dist-packages/data/models/tutorial_transformer_generator/tutorial_transformer_generator_v1.tar.gz]\n",
            "[ downloading: http://parl.ai/downloads/_models/tutorial_transformer_generator/tutorial_transformer_generator_v1.tar.gz to /usr/local/lib/python3.6/dist-packages/data/models/tutorial_transformer_generator/tutorial_transformer_generator_v1.tar.gz ]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading tutorial_transformer_generator_v1.tar.gz: 100%|██████████| 1.12G/1.12G [00:43<00:00, 25.8MB/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "unpacking tutorial_transformer_generator_v1.tar.gz\n",
            "[ warning: overriding opt['model_file'] to /usr/local/lib/python3.6/dist-packages/data/models/tutorial_transformer_generator/model (previously: /checkpoint/roller/20190909/cleanreddit/585/model )]\n",
            "[ Using CUDA ]\n",
            "Dictionary: loading dictionary from /usr/local/lib/python3.6/dist-packages/data/models/tutorial_transformer_generator/model.dict\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/parlai/utils/fp16.py:144: UserWarning: You set --fp16 true with --fp16-impl apex, but fp16 with apex is unavailable. To use apex fp16, please install APEX from https://github.com/NVIDIA/apex.\n",
            "  'You set --fp16 true with --fp16-impl apex, but fp16 '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[ num words =  54944 ]\n",
            "[TransformerGenerator: full interactive mode on.]\n",
            "Total parameters: 87,508,992 (87,508,992 trainable)\n",
            "[ Loading existing model params from /usr/local/lib/python3.6/dist-packages/data/models/tutorial_transformer_generator/model ]\n",
            "[ optional arguments: ] \n",
            "[  display_examples: False ]\n",
            "[  display_ignore_fields: label_candidates,text_candidates ]\n",
            "[  display_prettify: False ]\n",
            "[  interactive_task: True ]\n",
            "[ Main ParlAI Arguments: ] \n",
            "[  batchsize: 48 ]\n",
            "[  datapath: /usr/local/lib/python3.6/dist-packages/data ]\n",
            "[  datatype: train:stream ]\n",
            "[  download_path: /private/home/roller/working/parlai/downloads ]\n",
            "[  dynamic_batching: None ]\n",
            "[  hide_labels: False ]\n",
            "[  image_mode: raw ]\n",
            "[  init_opt: None ]\n",
            "[  multitask_weights: [1] ]\n",
            "[  numthreads: 1 ]\n",
            "[  show_advanced_args: False ]\n",
            "[  task: internal:new_reddit:presorted ]\n",
            "[ ParlAI Model Arguments: ] \n",
            "[  dict_class: parlai.core.dict:DictionaryAgent ]\n",
            "[  init_model: None ]\n",
            "[  model: transformer/generator ]\n",
            "[  model_file: /usr/local/lib/python3.6/dist-packages/data/models/tutorial_transformer_generator/model ]\n",
            "[ Local Human Arguments: ] \n",
            "[  local_human_candidates_file: None ]\n",
            "[  single_turn: False ]\n",
            "[ ParlAI Image Preprocessing Arguments: ] \n",
            "[  image_cropsize: 224 ]\n",
            "[  image_size: 256 ]\n",
            "[ Transformer Arguments: ] \n",
            "[  activation: gelu ]\n",
            "[  attention_dropout: 0.0 ]\n",
            "[  dropout: 0.1 ]\n",
            "[  embedding_size: 512 ]\n",
            "[  embeddings_scale: True ]\n",
            "[  ffn_size: 2048 ]\n",
            "[  learn_positional_embeddings: True ]\n",
            "[  model_parallel: False ]\n",
            "[  n_decoder_layers: -1 ]\n",
            "[  n_encoder_layers: -1 ]\n",
            "[  n_heads: 16 ]\n",
            "[  n_layers: 8 ]\n",
            "[  n_positions: 512 ]\n",
            "[  n_segments: 0 ]\n",
            "[  output_scaling: 1.0 ]\n",
            "[  relu_dropout: 0.0 ]\n",
            "[  share_word_embeddings: True ]\n",
            "[  variant: xlm ]\n",
            "[ Torch Generator Agent: ] \n",
            "[  beam_block_ngram: 3 ]\n",
            "[  beam_context_block_ngram: 3 ]\n",
            "[  beam_delay: 30 ]\n",
            "[  beam_length_penalty: 0.65 ]\n",
            "[  beam_min_length: 10 ]\n",
            "[  beam_size: 8 ]\n",
            "[  compute_tokenized_bleu: False ]\n",
            "[  inference: beam ]\n",
            "[  skip_generation: False ]\n",
            "[  temperature: 1.0 ]\n",
            "[  topk: 10 ]\n",
            "[  topp: 0.9 ]\n",
            "[ TorchAgent Arguments: ] \n",
            "[  add_p1_after_newln: False ]\n",
            "[  delimiter: \n",
            " ]\n",
            "[  embedding_projection: random ]\n",
            "[  embedding_type: random ]\n",
            "[  force_fp16_tokens: True ]\n",
            "[  fp16: True ]\n",
            "[  fp16_impl: apex ]\n",
            "[  gpu: 0 ]\n",
            "[  history_add_global_end_token: None ]\n",
            "[  history_size: -1 ]\n",
            "[  interactive_mode: True ]\n",
            "[  label_truncate: 128 ]\n",
            "[  no_cuda: False ]\n",
            "[  person_tokens: False ]\n",
            "[  rank_candidates: False ]\n",
            "[  split_lines: False ]\n",
            "[  text_truncate: 512 ]\n",
            "[  truncate: -1 ]\n",
            "[  use_reply: label ]\n",
            "[ Optimizer Arguments: ] \n",
            "[  adafactor_eps: (1e-30, 0.001) ]\n",
            "[  adam_eps: 1e-06 ]\n",
            "[  betas: [0.9, 0.98] ]\n",
            "[  gradient_clip: 10.0 ]\n",
            "[  learningrate: 0.0005 ]\n",
            "[  momentum: 0 ]\n",
            "[  nesterov: True ]\n",
            "[  nus: [0.7] ]\n",
            "[  optimizer: fused_adam ]\n",
            "[  weight_decay: 0.01 ]\n",
            "[ Dictionary Arguments: ] \n",
            "[  bpe_debug: False ]\n",
            "[  dict_endtoken: __end__ ]\n",
            "[  dict_file: /usr/local/lib/python3.6/dist-packages/data/models/tutorial_transformer_generator/model.dict ]\n",
            "[  dict_initpath: None ]\n",
            "[  dict_language: english ]\n",
            "[  dict_lower: True ]\n",
            "[  dict_max_ngram_size: -1 ]\n",
            "[  dict_maxtokens: -1 ]\n",
            "[  dict_minfreq: 0 ]\n",
            "[  dict_nulltoken: __null__ ]\n",
            "[  dict_starttoken: __start__ ]\n",
            "[  dict_textfields: text,labels ]\n",
            "[  dict_tokenizer: bpe ]\n",
            "[  dict_unktoken: __unk__ ]\n",
            "[ BPEHelper Arguments: ] \n",
            "[  bpe_add_prefix_space: None ]\n",
            "[  bpe_merge: None ]\n",
            "[  bpe_vocab: None ]\n",
            "[ Learning Rate Scheduler: ] \n",
            "[  invsqrt_lr_decay_gamma: -1 ]\n",
            "[  lr_scheduler: invsqrt ]\n",
            "[  lr_scheduler_decay: 0.5 ]\n",
            "[  lr_scheduler_patience: 3 ]\n",
            "[  max_lr_steps: -1 ]\n",
            "[  update_freq: 1 ]\n",
            "[  warmup_rate: 0.0001 ]\n",
            "[  warmup_updates: 20000 ]\n",
            "\u001b[1;31mEnter [DONE] if you want to end the episode, [EXIT] to quit.\u001b[0;0m\n",
            "[creating task(s): interactive]\n",
            "\u001b[0mEnter Your Message:\u001b[0;0m hi, how are you today?\n",
            "\u001b[0;34m[TransformerGenerator]:\u001b[0;0m \u001b[1mi ' m doing well , how about you ?\u001b[0;0m\n",
            "\u001b[0mEnter Your Message:\u001b[0;0m i'm great, just giving a tutorial!\n",
            "\u001b[0;34m[TransformerGenerator]:\u001b[0;0m \u001b[1mthat ' s awesome ! what kind of tutorial ?\u001b[0;0m\n",
            "\u001b[0mEnter Your Message:\u001b[0;0m a chatbot tutorial!\n",
            "\u001b[0;34m[TransformerGenerator]:\u001b[0;0m \u001b[1mwhat ' s the name of the chatbot ?\u001b[0;0m\n",
            "\u001b[0mEnter Your Message:\u001b[0;0m [EXIT]\n",
            "\n",
            "CHAT DONE \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3hfUEgovmWay"
      },
      "source": [
        "The same on the command line:\n",
        "```bash\n",
        "python -m parlai.scripts.interactive --model-file zoo:tutorial_transformer_generator/model\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o_hGrZGGmaWF"
      },
      "source": [
        "# Taking a look at some data\n",
        "\n",
        "We can look at look into a specific dataset. Let's look into the \"empathetic dialogues\" dataset, which aims to teach models how to respond with text expressing the appropriate emotion. We have over existing 80 datasets in ParlAI. You can take a full look in our [task list](https://parl.ai/docs/tasks.html)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AqckSXqlmWuT",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 377
        },
        "outputId": "ae007526-d913-48a1-f411-f511cbf6b483"
      },
      "source": [
        "# The display_data script is used to show the contents of a particular task.\n",
        "# By default, we show the train\n",
        "from parlai.scripts.display_data import DisplayData\n",
        "DisplayData.main(task='empathetic_dialogues', num_examples=5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\rDownloading empatheticdialogues.tar.gz: 0.00B [00:00, ?B/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[creating task(s): empathetic_dialogues]\n",
            "[EmpatheticDialoguesTeacher] Only use experiencer side? False, datatype: train:ordered\n",
            "[building data: /usr/local/lib/python3.6/dist-packages/data/empatheticdialogues]\n",
            "[ downloading: http://parl.ai/downloads/empatheticdialogues/empatheticdialogues.tar.gz to /usr/local/lib/python3.6/dist-packages/data/empatheticdialogues/empatheticdialogues.tar.gz ]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading empatheticdialogues.tar.gz: 100%|██████████| 28.0M/28.0M [00:02<00:00, 11.8MB/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[ Checksum Successful ]\n",
            "unpacking empatheticdialogues.tar.gz\n",
            "\u001b[1;31m- - - NEW EPISODE: empathetic_dialogues - - -\u001b[0;0m\n",
            "\u001b[0mI remember going to see the fireworks with my best friend. It was the first time we ever spent time alone together. Although there was a lot of people, we felt like the only people in the world.\u001b[0;0m\n",
            "   \u001b[1;94mWas this a friend you were in love with, or just a best friend?\u001b[0;0m\n",
            "\u001b[0mThis was a best friend. I miss her.\u001b[0;0m\n",
            "   \u001b[1;94mWhere has she gone?\u001b[0;0m\n",
            "\u001b[0mWe no longer talk.\u001b[0;0m\n",
            "   \u001b[1;94mOh was this something that happened because of an argument?\u001b[0;0m\n",
            "\u001b[1;31m- - - NEW EPISODE: empathetic_dialogues - - -\u001b[0;0m\n",
            "\u001b[0mWas this a friend you were in love with, or just a best friend?\u001b[0;0m\n",
            "   \u001b[1;94mThis was a best friend. I miss her.\u001b[0;0m\n",
            "\u001b[0mWhere has she gone?\u001b[0;0m\n",
            "   \u001b[1;94mWe no longer talk.\u001b[0;0m\n",
            "[ loaded 39057 episodes with a total of 64636 examples ]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a9C6oHq87zGx"
      },
      "source": [
        "The black, unindented text is the _prompt_, while the blue text is the _label_. That is, the label is what we will be training the model to mimic.\n",
        "\n",
        "We can als ask to see fewer examples, and get them from the validation set instead."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RGNSBetWmfGF",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        },
        "outputId": "98be5478-8380-4fd0-93a1-a01253c33413"
      },
      "source": [
        "# we can instead ask to see fewer examples, and get them from the valid set.\n",
        "DisplayData.main(task='empathetic_dialogues', num_examples=3, datatype='valid')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[creating task(s): empathetic_dialogues]\n",
            "[EmpatheticDialoguesTeacher] Only use experiencer side? True, datatype: valid:ordered\n",
            "\u001b[1;31m- - - NEW EPISODE: empathetic_dialogues - - -\u001b[0;0m\n",
            "\u001b[0mToday,as i was leaving for work in the morning,i had a tire burst in the middle of a busy road. That scared the hell out of me!\u001b[0;0m\n",
            "   \u001b[1;94mAre you fine now?\u001b[0;0m\n",
            "\u001b[0mYeah,i'm doing alright now, but with minor injuries.\u001b[0;0m\n",
            "   \u001b[1;94mCool :) Is your car damaged a lot?\u001b[0;0m\n",
            "\u001b[1;31m- - - NEW EPISODE: empathetic_dialogues - - -\u001b[0;0m\n",
            "\u001b[0mA few weeks ago, I was walking through my hallway, minding my own business, when all of a sudden a hand reached out from under a table and grabbed my ankle. I was so suprised. I thought i was got. Turns out, it was my son. \u001b[0;0m\n",
            "   \u001b[1;94mThat's funny, hope he didn't give you a heart attack.\u001b[0;0m\n",
            "[ loaded 2769 episodes with a total of 5738 examples ]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IVSrgRrEmdS-"
      },
      "source": [
        "On the command line:\n",
        "```bash\n",
        "python -m parlai.scripts.display_data --task empathetic_dialogues\n",
        "```\n",
        "or a bit shorter\n",
        "```\n",
        "python -m parlai.scripts.display_data -t empathetic_dialogues\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9_M8Zr86n2_G"
      },
      "source": [
        "# Training a model\n",
        "\n",
        "Well it's one thing looking at data, but what if we want to train our own model (from scratch)? Let's train a very simple seq2seq LSTM with attention, to respond to empathetic dialogues.\n",
        "\n",
        "To get some extra performance, we'll initialize using GloVe embeddings, but we will cap the training time to 2 minutes for this tutorial. It won't perform very well, but that's okay."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pBhVQycSn2q_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "2096b88f-450b-47fa-a83b-177a06a8acf2"
      },
      "source": [
        "# we'll save it in the \"from_scratch_model\" directory\n",
        "!rm -rf from_scratch_model\n",
        "!mkdir -p from_scratch_model\n",
        "\n",
        "from parlai.scripts.train_model import TrainModel\n",
        "TrainModel.main(\n",
        "    # we MUST provide a filename\n",
        "    model_file='from_scratch_model/model',\n",
        "    # train on empathetic dialogues\n",
        "    task='empathetic_dialogues',\n",
        "    # limit training time to 2 minutes, and a batchsize of 16\n",
        "    max_train_time=2 * 60,\n",
        "    batchsize=16,\n",
        "    \n",
        "    # we specify the model type as seq2seq\n",
        "    model='seq2seq',\n",
        "    # some hyperparamter choices. We'll use attention. We could use pretrained\n",
        "    # embeddings too, with embedding_type='fasttext', but they take a long\n",
        "    # time to download.\n",
        "    attention='dot',\n",
        "    # tie the word embeddings of the encoder/decoder/softmax.\n",
        "    lookuptable='all',\n",
        "    # truncate text and labels at 64 tokens, for memory and time savings\n",
        "    truncate=64,\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[ building dictionary first... ]\n",
            "[creating task(s): empathetic_dialogues]\n",
            "[EmpatheticDialoguesTeacher] Only use experiencer side? False, datatype: train:ordered:stream\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Building dictionary: 100%|██████████| 64.6k/64.6k [00:02<00:00, 27.9kex/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Dictionary: saving dictionary to from_scratch_model/model.dict\n",
            "[ dictionary built with 22419 tokens in 0s ]\n",
            "[ no model with opt yet at: from_scratch_model/model(.opt) ]\n",
            "[ Using CUDA ]\n",
            "Dictionary: loading dictionary from from_scratch_model/model.dict\n",
            "[ num words =  22419 ]\n",
            "Total parameters: 3,453,203 (3,453,203 trainable)\n",
            "[creating task(s): empathetic_dialogues]\n",
            "[EmpatheticDialoguesTeacher] Only use experiencer side? False, datatype: train\n",
            "[ training... ]\n",
            "[ time:10.0s total_exs:2112 epochs:0.03 ]\n",
            "    clip  exs  gnorm  gpu_mem  loss  lr  ppl  token_acc  total_train_updates   tpb  updates\n",
            "       1 2112  1.027   .05203 8.965   1 7828     .09277                  132 256.4      132\n",
            "\n",
            "[ time:20.0s total_exs:4160 epochs:0.06 ]\n",
            "    clip  exs  gnorm  gpu_mem  loss  lr  ppl  token_acc  total_train_updates  tpb  updates\n",
            "       1 2048  1.114   .05203  8.38   1 4358      .1307                  260  255      128\n",
            "\n",
            "[ time:30.0s total_exs:6240 epochs:0.1 ]\n",
            "    clip  exs  gnorm  gpu_mem  loss  lr  ppl  token_acc  total_train_updates   tpb  updates\n",
            "       1 2080  1.149   .05203 8.089   1 3257      .1438                  390 257.5      130\n",
            "\n",
            "[ time:40.0s total_exs:8320 epochs:0.13 ]\n",
            "    clip  exs  gnorm  gpu_mem  loss  lr  ppl  token_acc  total_train_updates   tpb  updates\n",
            "       1 2080  1.175   .05203 7.848   1 2560      .1546                  520 259.4      130\n",
            "\n",
            "[ time:50.0s total_exs:10432 epochs:0.16 ]\n",
            "    clip  exs  gnorm  gpu_mem  loss  lr  ppl  token_acc  total_train_updates   tpb  updates\n",
            "       1 2112  1.207   .05204 7.648   1 2096      .1618                  652 257.5      132\n",
            "\n",
            "[ time:60.0s total_exs:12400 epochs:0.19 ]\n",
            "    clip  exs  gnorm  gpu_mem  loss  lr  ppl  token_acc  total_train_updates   tpb  updates\n",
            "       1 1968  1.219   .05203 7.496   1 1802      .1639                  775 264.9      123\n",
            "\n",
            "[ time:70.0s total_exs:14400 epochs:0.22 ]\n",
            "    clip  exs  gnorm  gpu_mem  loss  lr  ppl  token_acc  total_train_updates   tpb  updates\n",
            "       1 2000  1.253   .05203 7.358   1 1568      .1686                  900 263.5      125\n",
            "\n",
            "[ time:80.0s total_exs:16512 epochs:0.26 ]\n",
            "    clip  exs  gnorm  gpu_mem  loss  lr  ppl  token_acc  total_train_updates   tpb  updates\n",
            "       1 2112  1.291   .05203 7.219   1 1365      .1748                 1032 256.8      132\n",
            "\n",
            "[ time:90.0s total_exs:18544 epochs:0.29 ]\n",
            "    clip  exs  gnorm  gpu_mem  loss  lr  ppl  token_acc  total_train_updates   tpb  updates\n",
            "       1 2032  1.293   .05204 7.123   1 1240      .1806                 1159 261.7      127\n",
            "\n",
            "[ time:100.0s total_exs:20592 epochs:0.32 ]\n",
            "    clip  exs  gnorm  gpu_mem  loss  lr  ppl  token_acc  total_train_updates  tpb  updates\n",
            "       1 2048  1.305   .05204 6.993   1 1089      .1865                 1287  258      128\n",
            "\n",
            "[ time:110.0s total_exs:22640 epochs:0.35 ]\n",
            "    clip  exs  gnorm  gpu_mem  loss  lr  ppl  token_acc  total_train_updates   tpb  updates\n",
            "       1 2048  1.302   .05203 6.988   1 1083      .1838                 1415 261.6      128\n",
            "\n",
            "[ max_train_time elapsed:120.07246494293213s ]\n",
            "[ Using CUDA ]\n",
            "Dictionary: loading dictionary from from_scratch_model/model.dict\n",
            "[ num words =  22419 ]\n",
            "Total parameters: 3,453,203 (3,453,203 trainable)\n",
            "[ Loading existing model params from from_scratch_model/model ]\n",
            "[creating task(s): empathetic_dialogues]\n",
            "[EmpatheticDialoguesTeacher] Only use experiencer side? True, datatype: valid\n",
            "[ running eval: valid ]\n",
            "[ eval completed in 53.38s ]\n",
            "valid:\n",
            "    accuracy    bleu-4  exs    f1  gpu_mem  loss  lr   ppl  token_acc  total_train_updates   tpb\n",
            "           0 6.721e-07 5738 .1245   .05125 6.564   1 709.1      .2123                 1539 249.1\n",
            "\n",
            "[creating task(s): empathetic_dialogues]\n",
            "[EmpatheticDialoguesTeacher] Only use experiencer side? True, datatype: test\n",
            "[ running eval: test ]\n",
            "[ eval completed in 49.76s ]\n",
            "test:\n",
            "    accuracy    bleu-4  exs    f1  gpu_mem  loss  lr   ppl  token_acc  total_train_updates   tpb\n",
            "           0 1.166e-05 5259 .1255   .05127 6.574   1 716.3      .2100                 1539 252.6\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SvA77Zwkoviq"
      },
      "source": [
        "Our perplexity and F1 (word overlap) scores are pretty bad, and our BLEU-4 score is nearly 0. That's okay, we would normally want to train for well over an hour. Feel free to change the max_train_time above."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_QTiTn7aoxv9"
      },
      "source": [
        "## Performance is pretty bad there. Can we improve it?\n",
        "\n",
        "The easiest way to improve it is to *initialize* using a *pretrained model*, utilizing *transfer learning*. Let's use the one from the interactive session at the beginning of the chat!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O2Jt9bHTn1dP",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "b296910d-dcd6-477a-a27b-2d34f9326af5"
      },
      "source": [
        "!rm -rf from_pretrained\n",
        "!mkdir -p from_pretrained\n",
        "\n",
        "TrainModel.main(\n",
        "    # similar to before\n",
        "    task='empathetic_dialogues', \n",
        "    model='transformer/generator',\n",
        "    model_file='from_pretrained/model',\n",
        "    \n",
        "    # initialize with a pretrained model\n",
        "    init_model='zoo:tutorial_transformer_generator/model',\n",
        "    \n",
        "    # arguments we get from the pretrained model.\n",
        "    # Unfortunately, these must be looked up separately for each model.\n",
        "    n_heads=16, n_layers=8, n_positions=512, text_truncate=512,\n",
        "    label_truncate=128, ffn_size=2048, embedding_size=512,\n",
        "    activation='gelu', variant='xlm',\n",
        "    dict_lower=True, dict_tokenizer='bpe',\n",
        "    dict_file='zoo:tutorial_transformer_generator/model.dict',\n",
        "    learn_positional_embeddings=True,\n",
        "    \n",
        "    # some training arguments, specific to this fine-tuning\n",
        "    # use a small learning rate with ADAM optimizer\n",
        "    lr=1e-5, optimizer='adam',\n",
        "    warmup_updates=100,\n",
        "    # early stopping on perplexity\n",
        "    validation_metric='ppl',\n",
        "    # train at most 10 minutes, and validate every 0.25 epochs\n",
        "    max_train_time=600, validation_every_n_epochs=0.25,\n",
        "    \n",
        "    # depend on your gpu. If you have a V100, this is good\n",
        "    batchsize=12, fp16=True, fp16_impl='mem_efficient',\n",
        "    \n",
        "    # speeds up validation\n",
        "    skip_generation=True,\n",
        "    \n",
        "    # helps us cram more examples into our gpu at a time\n",
        "    dynamic_batching='full',\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[ building dictionary first... ]\n",
            "[ dictionary already built .]\n",
            "[ no model with opt yet at: from_pretrained/model(.opt) ]\n",
            "\n",
            "***************************************************************************\n",
            "[ WARNING ] : your model is being loaded with opts that do not exist in the model you are initializing the weights with: dynamic_batching: full,datapath: /usr/local/lib/python3.6/dist-packages/data,train_experiencer_only: False,remove_political_convos: False,n_encoder_layers: -1,n_decoder_layers: -1,model_parallel: False,beam_length_penalty: 0.65,topk: 10,topp: 0.9,beam_delay: 30,temperature: 1.0,compute_tokenized_bleu: False,interactive_mode: False,fp16_impl: mem_efficient,force_fp16_tokens: False,adafactor_eps: (1e-30, 0.001),history_add_global_end_token: None,bpe_vocab: None,bpe_merge: None,bpe_add_prefix_space: None,max_lr_steps: -1,invsqrt_lr_decay_gamma: -1,parlai_home: /usr/local/lib/python3.6/dist-packages\n",
            "\n",
            "***************************************************************************\n",
            "[ WARNING ] : your model is being loaded with opts that differ from the model you are initializing the weights with. Add the following args to your run command to change this: \n",
            "\n",
            "--task internal:new-reddit:presorted --download-path /private/home/roller/working/parlai/downloads --datatype train:stream --batchsize 48 --num-epochs 5.0 --max-train-time -1 --validation-every-n-secs 1800.0 --save-after-valid True --validation-every-n-epochs -1 --validation-max-exs 9920 --short-final-eval True --validation-patience 0 --validation-metric-mode min --dict-build-first True --load-from-checkpoint True --numworkers 4 --pytorch-preprocess False --pytorch-teacher-batch-sort False --batch-sort-cache-type pop --batch-length-range 5 --shuffle False --batch-sort-field text --pytorch-context-length -1 --pytorch-include-labels True --log-every-n-secs 30.0 --distributed-world-size 64 --verbose False --port 61337 --dropout 0.1 --beam-size 8 --beam-min-n-best 3 --beam-min-length 10 --skip-generation False --inference beam --optimizer fused-adam --learningrate 0.0005 --gradient-clip 10.0 --adam-eps 1e-06 --betas 0.9,0.98 --weight-decay 0.01 --lr-scheduler invsqrt --warmup-updates 20000 --gpu 0 --beam-block-ngram 3 --beam-context-block-ngram 3\n",
            "***************************************************************************\n",
            "[ Using CUDA ]\n",
            "Dictionary: loading dictionary from /usr/local/lib/python3.6/dist-packages/data/models/tutorial_transformer_generator/model.dict\n",
            "[ num words =  54944 ]\n",
            "Total parameters: 87,508,992 (87,508,992 trainable)\n",
            "[ Loading existing model params from /usr/local/lib/python3.6/dist-packages/data/models/tutorial_transformer_generator/model ]\n",
            "WARNING: not loading optim state since optim class changed.\n",
            "[creating task(s): empathetic_dialogues]\n",
            "[EmpatheticDialoguesTeacher] Only use experiencer side? False, datatype: train\n",
            "[ training... ]\n",
            "[ time:10.0s total_exs:2400 epochs:0.04 ]\n",
            "    clip  exs  fp16_loss_scalar  gnorm  gpu_mem  loss        lr  ppl  token_acc  total_train_updates  tpb  updates\n",
            "   .9474 2400             65536  4.133    .9024 2.833 1.901e-06   17      .3994                   19 1996       19\n",
            "\n",
            "[ time:20.0s total_exs:4316 epochs:0.07 ]\n",
            "    clip  exs  fp16_loss_scalar  gnorm  gpu_mem  loss        lr   ppl  token_acc  total_train_updates  tpb  updates\n",
            "   .9545 1916             65536  4.157    1.076 2.863 4.101e-06 17.51      .3948                   41 1487       22\n",
            "\n",
            "[ time:31.0s total_exs:6188 epochs:0.1 ]\n",
            "    clip  exs  fp16_loss_scalar  gnorm  gpu_mem  loss      lr   ppl  token_acc  total_train_updates  tpb  updates\n",
            "       1 1872             65536  4.304    .7890 2.818 6.2e-06 16.74      .3934                   62 1494       21\n",
            "\n",
            "[ time:41.0s total_exs:8356 epochs:0.13 ]\n",
            "    clip  exs  fp16_loss_scalar  gnorm  gpu_mem  loss      lr   ppl  token_acc  total_train_updates  tpb  updates\n",
            "       1 2168             65536  4.151    .7935 2.802 8.4e-06 16.48      .3989                   84 1665       22\n",
            "\n",
            "[ time:51.0s total_exs:10424 epochs:0.16 ]\n",
            "    clip  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr   ppl  token_acc  total_train_updates  tpb  updates\n",
            "       1 2068             65536  4.095    .7935 2.768 1e-05 15.93      .4004                  105 1598       21\n",
            "\n",
            "[ time:62.0s total_exs:12404 epochs:0.19 ]\n",
            "    clip  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr   ppl  token_acc  total_train_updates  tpb  updates\n",
            "       1 1980             65536  4.205    .7935 2.711 1e-05 15.05      .4072                  126 1574       21\n",
            "\n",
            "[ time:72.0s total_exs:14316 epochs:0.22 ]\n",
            "    clip  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr   ppl  token_acc  total_train_updates  tpb  updates\n",
            "       1 1912             65536  4.136    .7935 2.755 1e-05 15.72      .4055                  146 1586       20\n",
            "\n",
            "[ time:82.0s total_exs:16068 epochs:0.25 ]\n",
            "    clip  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr   ppl  token_acc  total_train_updates  tpb  updates\n",
            "   .9500 1752             65536  3.758    .7935 2.742 1e-05 15.52      .4044                  166 1511       20\n",
            "\n",
            "[ time:83.0s total_exs:16216 epochs:0.25 ]\n",
            "    clip  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr   ppl  token_acc  total_train_updates   tpb  updates\n",
            "       1  148             65536  4.756    .7935 2.578 1e-05 13.17      .4299                  168 980.5        2\n",
            "\n",
            "[creating task(s): empathetic_dialogues]\n",
            "[EmpatheticDialoguesTeacher] Only use experiencer side? True, datatype: valid\n",
            "[ running eval: valid ]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/parlai/core/torch_generator_agent.py:739: RuntimeWarning: --skip-generation does not produce accurate metrics beyond ppl\n",
            "  RuntimeWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[ eval completed in 10.95s ]\n",
            "valid:\n",
            "    exs  gpu_mem  loss    lr   ppl  token_acc  total_train_updates  tpb\n",
            "   5738    .7935 2.535 1e-05 12.61      .4311                  168 1392\n",
            "\n",
            "[ new best ppl: 12.61 ]\n",
            "[ saving best valid model: from_pretrained/model ]\n",
            "Dictionary: saving dictionary to from_pretrained/model.dict\n",
            "[ time:111.0s total_exs:18340 epochs:0.28 ]\n",
            "    clip  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr   ppl  token_acc  total_train_updates  tpb  updates\n",
            "       1 2124             65536  3.997    .7935  2.67 1e-05 14.44      .4143                  189 1629       21\n",
            "\n",
            "[ time:121.0s total_exs:20420 epochs:0.32 ]\n",
            "    clip  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ppl  token_acc  total_train_updates  tpb  updates\n",
            "       1 2080             65536  3.842    .7935 2.701 1e-05 14.9      .4116                  209 1627       20\n",
            "\n",
            "[ time:131.0s total_exs:22304 epochs:0.35 ]\n",
            "    clip  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr   ppl  token_acc  total_train_updates  tpb  updates\n",
            "       1 1884             65536  4.143    .7935 2.676 1e-05 14.53      .4153                  230 1554       21\n",
            "\n",
            "[ time:142.0s total_exs:24144 epochs:0.37 ]\n",
            "    clip  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr   ppl  token_acc  total_train_updates  tpb  updates\n",
            "       1 1840             65536  4.314    .7935 2.695 1e-05 14.81      .4094                  251 1483       21\n",
            "\n",
            "[ time:152.0s total_exs:26108 epochs:0.4 ]\n",
            "    clip  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr   ppl  token_acc  total_train_updates  tpb  updates\n",
            "       1 1964             65536      4    .7935 2.706 1e-05 14.97      .4115                  271 1696       20\n",
            "\n",
            "[ time:162.0s total_exs:28084 epochs:0.43 ]\n",
            "    clip  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ppl  token_acc  total_train_updates  tpb  updates\n",
            "       1 1976             65536   3.93    .7935  2.66 1e-05 14.3      .4167                  291 1582       20\n",
            "\n",
            "[ time:172.0s total_exs:29820 epochs:0.46 ]\n",
            "    clip  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr   ppl  token_acc  total_train_updates  tpb  updates\n",
            "   .9500 1736             65536  3.782    .7951 2.669 1e-05 14.42      .4159                  311 1487       20\n",
            "\n",
            "[ time:183.0s total_exs:31724 epochs:0.49 ]\n",
            "    clip  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr   ppl  token_acc  total_train_updates  tpb  updates\n",
            "   .9524 1904             65536  3.902    .7952 2.671 1e-05 14.45      .4182                  332 1513       21\n",
            "\n",
            "[ time:187.0s total_exs:32432 epochs:0.5 ]\n",
            "    clip  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr   ppl  token_acc  total_train_updates  tpb  updates\n",
            "       1  708             65536  4.363    .7951 2.697 1e-05 14.83      .4157                  340 1427        8\n",
            "\n",
            "[ running eval: valid ]\n",
            "[ eval completed in 11.02s ]\n",
            "valid:\n",
            "    exs  gpu_mem  loss    lr  ppl  token_acc  total_train_updates  tpb\n",
            "   5738    .7952 2.502 1e-05 12.2      .4368                  340 1371\n",
            "\n",
            "[ new best ppl: 12.2 (previous best was 12.61) ]\n",
            "[ saving best valid model: from_pretrained/model ]\n",
            "[ time:212.0s total_exs:34316 epochs:0.53 ]\n",
            "    clip  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr   ppl  token_acc  total_train_updates  tpb  updates\n",
            "       1 1884             65536  4.172    .7951 2.698 1e-05 14.84      .4151                  360 1654       20\n",
            "\n",
            "[ time:222.0s total_exs:36112 epochs:0.56 ]\n",
            "    clip  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr   ppl  token_acc  total_train_updates  tpb  updates\n",
            "       1 1796             65536  4.279    .7951 2.676 1e-05 14.53      .4165                  380 1506       20\n",
            "\n",
            "[ time:233.0s total_exs:38132 epochs:0.59 ]\n",
            "    clip  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr   ppl  token_acc  total_train_updates  tpb  updates\n",
            "       1 2020             65536  3.964    .7951 2.629 1e-05 13.86      .4204                  401 1607       21\n",
            "\n",
            "[ time:243.0s total_exs:40184 epochs:0.62 ]\n",
            "    clip  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ppl  token_acc  total_train_updates  tpb  updates\n",
            "   .9500 2052             65536  3.638    .7951 2.674 1e-05 14.5      .4165                  421 1672       20\n",
            "\n",
            "[ time:253.0s total_exs:42144 epochs:0.65 ]\n",
            "    clip  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ppl  token_acc  total_train_updates  tpb  updates\n",
            "   .9545 1960             65536  4.016    .7951 2.639 1e-05   14      .4216                  443 1462       22\n",
            "\n",
            "[ time:263.0s total_exs:44112 epochs:0.68 ]\n",
            "    clip  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr   ppl  token_acc  total_train_updates  tpb  updates\n",
            "       1 1968             65536  4.199    .7951 2.665 1e-05 14.37      .4158                  463 1616       20\n",
            "\n",
            "[ time:274.0s total_exs:45636 epochs:0.71 ]\n",
            "    clip  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr   ppl  token_acc  total_train_updates  tpb  updates\n",
            "       1 1524             65536  4.868    .7951 2.668 1e-05 14.41      .4116                  484 1330       21\n",
            "\n",
            "[ time:284.0s total_exs:47624 epochs:0.74 ]\n",
            "    clip  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr   ppl  token_acc  total_train_updates  tpb  updates\n",
            "       1 1988             65536  4.121    .7961 2.656 1e-05 14.24      .4194                  505 1564       21\n",
            "\n",
            "[ time:289.0s total_exs:48608 epochs:0.75 ]\n",
            "    clip  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr   ppl  token_acc  total_train_updates  tpb  updates\n",
            "       1  984             65536  4.008    .7961 2.615 1e-05 13.67      .4243                  515 1608       10\n",
            "\n",
            "[ running eval: valid ]\n",
            "[ eval completed in 10.53s ]\n",
            "valid:\n",
            "    exs  gpu_mem  loss    lr   ppl  token_acc  total_train_updates  tpb\n",
            "   5738    .7962 2.487 1e-05 12.03      .4396                  515 1413\n",
            "\n",
            "[ new best ppl: 12.03 (previous best was 12.2) ]\n",
            "[ saving best valid model: from_pretrained/model ]\n",
            "[ time:314.0s total_exs:50600 epochs:0.78 ]\n",
            "    clip  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr   ppl  token_acc  total_train_updates  tpb  updates\n",
            "   .9524 1992             65536  4.034    .9784 2.672 1e-05 14.47      .4167                  536 1575       21\n",
            "\n",
            "[ time:324.0s total_exs:52624 epochs:0.81 ]\n",
            "    clip  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr   ppl  token_acc  total_train_updates  tpb  updates\n",
            "   .9524 2024             65536  3.749    .9784 2.669 1e-05 14.43      .4135                  557 1639       21\n",
            "\n",
            "[ time:334.0s total_exs:54384 epochs:0.84 ]\n",
            "    clip  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr   ppl  token_acc  total_train_updates  tpb  updates\n",
            "       1 1760             65536  4.334    .9784 2.647 1e-05 14.11      .4204                  578 1521       21\n",
            "\n",
            "[ time:345.0s total_exs:56268 epochs:0.87 ]\n",
            "    clip  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr   ppl  token_acc  total_train_updates  tpb  updates\n",
            "       1 1884             65536  4.176    .9784 2.643 1e-05 14.06      .4179                  598 1527       20\n",
            "\n",
            "[ time:355.0s total_exs:58552 epochs:0.91 ]\n",
            "    clip  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr   ppl  token_acc  total_train_updates  tpb  updates\n",
            "       1 2284             65536  3.952    .9784 2.654 1e-05 14.21      .4177                  619 1787       21\n",
            "\n",
            "[ time:366.0s total_exs:60572 epochs:0.94 ]\n",
            "    clip  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr   ppl  token_acc  total_train_updates  tpb  updates\n",
            "       1 2020             65536  4.301    .9784 2.624 1e-05 13.79      .4226                  641 1449       22\n",
            "\n",
            "[ time:376.0s total_exs:62404 epochs:0.97 ]\n",
            "    clip  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr   ppl  token_acc  total_train_updates  tpb  updates\n",
            "       1 1832             65536  4.146    .9784 2.678 1e-05 14.56      .4128                  661 1656       20\n",
            "\n",
            "[ time:386.0s total_exs:64396 epochs:1.0 ]\n",
            "    clip  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr   ppl  token_acc  total_train_updates  tpb  updates\n",
            "       1 1992             65536  4.062    .9784 2.608 1e-05 13.58      .4235                  680 1630       19\n",
            "\n",
            "[ time:388.0s total_exs:64796 epochs:1.0 ]\n",
            "    clip  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr   ppl  token_acc  total_train_updates  tpb  updates\n",
            "       1  400             65536  3.252    .9784 2.696 1e-05 14.82      .4030                  683 2195        3\n",
            "\n",
            "[ running eval: valid ]\n",
            "[ eval completed in 10.89s ]\n",
            "valid:\n",
            "    exs  gpu_mem  loss    lr   ppl  token_acc  total_train_updates  tpb\n",
            "   5738    .9785 2.478 1e-05 11.91      .4416                  683 1351\n",
            "\n",
            "[ new best ppl: 11.91 (previous best was 12.03) ]\n",
            "[ saving best valid model: from_pretrained/model ]\n",
            "[ time:414.0s total_exs:66664 epochs:1.03 ]\n",
            "    clip  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr   ppl  token_acc  total_train_updates  tpb  updates\n",
            "       1 1868             65536  4.206    .9784 2.627 1e-05 13.84      .4213                  704 1498       21\n",
            "\n",
            "[ time:424.0s total_exs:68604 epochs:1.06 ]\n",
            "    clip  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr   ppl  token_acc  total_train_updates  tpb  updates\n",
            "       1 1940             65536  4.209    .9784 2.612 1e-05 13.62      .4201                  724 1515       20\n",
            "\n",
            "[ time:434.0s total_exs:70480 epochs:1.09 ]\n",
            "    clip  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr   ppl  token_acc  total_train_updates  tpb  updates\n",
            "       1 1876             65536  4.161    .9784 2.663 1e-05 14.33      .4129                  745 1579       21\n",
            "\n",
            "[ time:444.0s total_exs:72456 epochs:1.12 ]\n",
            "    clip  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr   ppl  token_acc  total_train_updates  tpb  updates\n",
            "       1 1976             65536  4.051    .9784 2.605 1e-05 13.53      .4234                  766 1612       21\n",
            "\n",
            "[ time:455.0s total_exs:74432 epochs:1.15 ]\n",
            "    clip  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr   ppl  token_acc  total_train_updates  tpb  updates\n",
            "       1 1976             65536  4.259    .9784 2.626 1e-05 13.82      .4208                  788 1479       22\n",
            "\n",
            "[ time:465.0s total_exs:76468 epochs:1.18 ]\n",
            "    clip  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr   ppl  token_acc  total_train_updates  tpb  updates\n",
            "   .9565 2036             65536  3.956    .9784 2.674 1e-05 14.49      .4150                  811 1447       23\n",
            "\n",
            "[ time:475.0s total_exs:78428 epochs:1.21 ]\n",
            "    clip  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ppl  token_acc  total_train_updates  tpb  updates\n",
            "       1 1960             65536  4.101    .9784 2.618 1e-05 13.7      .4184                  833 1491       22\n",
            "\n",
            "[ time:485.0s total_exs:80484 epochs:1.25 ]\n",
            "    clip  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr   ppl  token_acc  total_train_updates  tpb  updates\n",
            "   .9545 2056             65536  3.875    .9784 2.631 1e-05 13.88      .4189                  855 1539       22\n",
            "\n",
            "[ time:488.0s total_exs:81032 epochs:1.25 ]\n",
            "    clip  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr   ppl  token_acc  total_train_updates  tpb  updates\n",
            "       1  548             65536  4.182    .9784 2.694 1e-05 14.79      .4086                  861 1871        6\n",
            "\n",
            "[ running eval: valid ]\n",
            "[ eval completed in 10.90s ]\n",
            "valid:\n",
            "    exs  gpu_mem  loss    lr   ppl  token_acc  total_train_updates  tpb\n",
            "   5738    .9785  2.47 1e-05 11.82      .4421                  861 1413\n",
            "\n",
            "[ new best ppl: 11.82 (previous best was 11.91) ]\n",
            "[ saving best valid model: from_pretrained/model ]\n",
            "[ time:514.0s total_exs:83004 epochs:1.28 ]\n",
            "    clip  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr   ppl  token_acc  total_train_updates  tpb  updates\n",
            "       1 1972             65536  4.243    .9784 2.653 1e-05 14.19      .4150                  883 1462       22\n",
            "\n",
            "[ time:525.0s total_exs:84904 epochs:1.31 ]\n",
            "    clip  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr   ppl  token_acc  total_train_updates  tpb  updates\n",
            "   .9545 1900             65536  4.139    .9784 2.641 1e-05 14.02      .4205                  905 1451       22\n",
            "\n",
            "[ time:535.0s total_exs:86840 epochs:1.34 ]\n",
            "    clip  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr   ppl  token_acc  total_train_updates  tpb  updates\n",
            "       1 1936             65536  3.878    .9784 2.591 1e-05 13.35      .4247                  925 1747       20\n",
            "\n",
            "[ time:545.0s total_exs:88624 epochs:1.37 ]\n",
            "    clip  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr   ppl  token_acc  total_train_updates  tpb  updates\n",
            "   .9500 1784             65536   3.87    .9784 2.601 1e-05 13.48      .4263                  945 1501       20\n",
            "\n",
            "[ time:556.0s total_exs:90756 epochs:1.4 ]\n",
            "    clip  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr   ppl  token_acc  total_train_updates  tpb  updates\n",
            "   .9545 2132             65536  3.901    .9784 2.621 1e-05 13.76      .4202                  967 1632       22\n",
            "\n",
            "[ time:566.0s total_exs:92604 epochs:1.43 ]\n",
            "    clip  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr   ppl  token_acc  total_train_updates  tpb  updates\n",
            "       1 1848             65536   4.28    .9784 2.587 1e-05 13.29      .4270                  987 1453       20\n",
            "\n",
            "[ time:576.0s total_exs:94784 epochs:1.47 ]\n",
            "    clip  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr   ppl  token_acc  total_train_updates  tpb  updates\n",
            "   .9524 2180             65536   3.78    .9784  2.62 1e-05 13.74      .4220                 1008 1769       21\n",
            "\n",
            "[ time:586.0s total_exs:96836 epochs:1.5 ]\n",
            "    clip  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr   ppl  token_acc  total_train_updates  tpb  updates\n",
            "   .9545 2052             65536  4.559    .9784 2.599 1e-05 13.44      .4241                 1030 1404       22\n",
            "\n",
            "[ time:589.0s total_exs:97200 epochs:1.5 ]\n",
            "    clip  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr   ppl  token_acc  total_train_updates  tpb  updates\n",
            "       1  364             65536      4    .9784 2.587 1e-05 13.28      .4300                 1034 1664        4\n",
            "\n",
            "[ running eval: valid ]\n",
            "[ eval completed in 10.65s ]\n",
            "valid:\n",
            "    exs  gpu_mem  loss    lr   ppl  token_acc  total_train_updates  tpb\n",
            "   5738    .9785 2.463 1e-05 11.74      .4423                 1034 1371\n",
            "\n",
            "[ new best ppl: 11.74 (previous best was 11.82) ]\n",
            "[ saving best valid model: from_pretrained/model ]\n",
            "[ max_train_time elapsed:604.4430022239685s ]\n",
            "[ warning: overriding opt['init_model'] to zoo:tutorial_transformer_generator/model (previously: /usr/local/lib/python3.6/dist-packages/data/models/tutorial_transformer_generator/model )]\n",
            "[ warning: overriding opt['optimizer'] to adam (previously: mem_eff_adam )]\n",
            "\n",
            "***************************************************************************\n",
            "[ WARNING ] : your model is being loaded with opts that do not exist in the model you are initializing the weights with: dynamic_batching: full,train_experiencer_only: False,remove_political_convos: False,n_encoder_layers: -1,n_decoder_layers: -1,model_parallel: False,beam_length_penalty: 0.65,topk: 10,topp: 0.9,beam_delay: 30,temperature: 1.0,compute_tokenized_bleu: False,fp16_impl: mem_efficient,force_fp16_tokens: True,adafactor_eps: 1e-30,0.001,history_add_global_end_token: None,bpe_vocab: None,bpe_merge: None,bpe_add_prefix_space: None,max_lr_steps: -1,invsqrt_lr_decay_gamma: -1,parlai_home: /usr/local/lib/python3.6/dist-packages,dict_loaded: True,datapath: /usr/local/lib/python3.6/dist-packages/data,interactive_mode: False\n",
            "\n",
            "***************************************************************************\n",
            "[ WARNING ] : your model is being loaded with opts that differ from the model you are initializing the weights with. Add the following args to your run command to change this: \n",
            "\n",
            "--task internal:new-reddit:presorted --download-path /private/home/roller/working/parlai/downloads --datatype train:stream --batchsize 48 --num-epochs 5.0 --max-train-time -1 --validation-every-n-secs 1800.0 --save-after-valid True --validation-every-n-epochs -1 --validation-max-exs 9920 --short-final-eval True --validation-patience 0 --validation-metric-mode min --dict-build-first True --load-from-checkpoint True --numworkers 4 --pytorch-preprocess False --pytorch-teacher-batch-sort False --batch-sort-cache-type pop --batch-length-range 5 --shuffle False --batch-sort-field text --pytorch-context-length -1 --pytorch-include-labels True --log-every-n-secs 30.0 --distributed-world-size 64 --verbose False --port 61337 --dropout 0.1 --beam-size 8 --beam-min-n-best 3 --beam-min-length 10 --skip-generation False --inference beam --optimizer fused-adam --learningrate 0.0005 --gradient-clip 10.0 --adam-eps 1e-06 --betas 0.9,0.98 --weight-decay 0.01 --lr-scheduler invsqrt --warmup-updates 20000 --gpu 0 --beam-block-ngram 3 --beam-context-block-ngram 3\n",
            "***************************************************************************\n",
            "[ Using CUDA ]\n",
            "Dictionary: loading dictionary from from_pretrained/model.dict\n",
            "[ num words =  54944 ]\n",
            "Total parameters: 87,508,992 (87,508,992 trainable)\n",
            "[ Loading existing model params from from_pretrained/model ]\n",
            "[creating task(s): empathetic_dialogues]\n",
            "[EmpatheticDialoguesTeacher] Only use experiencer side? True, datatype: valid\n",
            "[ running eval: valid ]\n",
            "[ eval completed in 10.79s ]\n",
            "valid:\n",
            "    exs  gpu_mem  loss    lr   ppl  token_acc  total_train_updates  tpb\n",
            "   5738    .9810 2.463 1e-05 11.74      .4423                 1034 1371\n",
            "\n",
            "[creating task(s): empathetic_dialogues]\n",
            "[EmpatheticDialoguesTeacher] Only use experiencer side? True, datatype: test\n",
            "[ running eval: test ]\n",
            "[ eval completed in 10.51s ]\n",
            "test:\n",
            "    exs  gpu_mem  loss    lr  ppl  token_acc  total_train_updates  tpb\n",
            "   5259    .9810 2.485 1e-05   12      .4392                 1034 1313\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0iBZXTLRvIjb"
      },
      "source": [
        "## Wow that's a lot of options? Where do I find more info?\n",
        "\n",
        "As you might have noticed, there are a LOT of options to ParlAI. You're best reading the [ParlAI docs](https://parl.ai/docs) to find a list of hyperparameters. We provide lists of the command-line args for both models\n",
        "\n",
        "You can get some guidance in this notebook by using:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Pl8VVl5plfm",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "453c1f82-a7ff-4df7-c7ad-f728c6e67854"
      },
      "source": [
        "# note that if you want to see model-specific arguments, you must specify a model name\n",
        "print(TrainModel.help(model='seq2seq'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "usage: TrainModel [-h] [-o INIT_OPT] [-v] [-t TASK]\n",
            "                  [-dt {train,train:stream,train:ordered,train:ordered:stream,train:stream:ordered,train:evalmode,train:evalmode:stream,train:evalmode:ordered,train:evalmode:ordered:stream,train:evalmode:stream:ordered,valid,valid:stream,test,test:stream}]\n",
            "                  [-nt NUMTHREADS] [-bs BATCHSIZE] [-dynb {None,batchsort,full}]\n",
            "                  [-dp DATAPATH] [-m MODEL] [-mf MODEL_FILE] [-im INIT_MODEL]\n",
            "                  [-et EVALTASK] [-eps NUM_EPOCHS] [-ttim MAX_TRAIN_TIME]\n",
            "                  [-vtim VALIDATION_EVERY_N_SECS] [-stim SAVE_EVERY_N_SECS]\n",
            "                  [-sval SAVE_AFTER_VALID] [-veps VALIDATION_EVERY_N_EPOCHS]\n",
            "                  [-vp VALIDATION_PATIENCE] [-vmt VALIDATION_METRIC]\n",
            "                  [-vmm {max,min}] [-mcs METRICS] [-micro AGGREGATE_MICRO]\n",
            "                  [-tblog TENSORBOARD_LOG] [-hs HIDDENSIZE] [-esz EMBEDDINGSIZE]\n",
            "                  [-nl NUMLAYERS] [-dr DROPOUT] [-bi BIDIRECTIONAL]\n",
            "                  [-att {none,concat,general,dot,local}]\n",
            "                  [-attl ATTENTION_LENGTH] [--attention-time {pre,post}]\n",
            "                  [-rnn {rnn,gru,lstm}] [-dec {same,shared}]\n",
            "                  [-lt {unique,enc_dec,dec_out,all}] [-soft NUMSOFTMAX]\n",
            "                  [-idr INPUT_DROPOUT] [--beam-size BEAM_SIZE]\n",
            "                  [--beam-min-length BEAM_MIN_LENGTH]\n",
            "                  [--beam-context-block-ngram BEAM_CONTEXT_BLOCK_NGRAM]\n",
            "                  [--beam-block-ngram BEAM_BLOCK_NGRAM]\n",
            "                  [--beam-length-penalty BEAM_LENGTH_PENALTY]\n",
            "                  [--inference {topk,beam,nucleus,delayedbeam,greedy}]\n",
            "                  [--topk TOPK] [--topp TOPP] [--beam-delay BEAM_DELAY]\n",
            "                  [--temperature TEMPERATURE]\n",
            "                  [--compute-tokenized-bleu COMPUTE_TOKENIZED_BLEU]\n",
            "                  [-i INTERACTIVE_MODE]\n",
            "                  [-emb {random,glove,glove-fixed,fasttext,fasttext-fixed,fasttext_cc,fasttext_cc-fixed}]\n",
            "                  [-embp EMBEDDING_PROJECTION] [--fp16 FP16]\n",
            "                  [--fp16-impl {apex,mem_efficient}]\n",
            "                  [-opt {adadelta,adagrad,adam,adamw,sparseadam,adamax,asgd,sgd,rprop,rmsprop,optimizer,lbfgs,mem_eff_adam,adafactor}]\n",
            "                  [-lr LEARNINGRATE] [-clip GRADIENT_CLIP]\n",
            "                  [--adafactor-eps ADAFACTOR_EPS] [-mom MOMENTUM]\n",
            "                  [--nesterov NESTEROV] [-nu NUS] [-beta BETAS]\n",
            "                  [-wdecay WEIGHT_DECAY] [-rc RANK_CANDIDATES] [-tr TRUNCATE]\n",
            "                  [--text-truncate TEXT_TRUNCATE]\n",
            "                  [--label-truncate LABEL_TRUNCATE] [-histsz HISTORY_SIZE]\n",
            "                  [-pt PERSON_TOKENS] [--split-lines SPLIT_LINES]\n",
            "                  [--delimiter DELIMITER] [-gpu GPU | --no-cuda]\n",
            "                  [--bpe-vocab BPE_VOCAB] [--bpe-merge BPE_MERGE]\n",
            "                  [--lr-scheduler {reduceonplateau,none,fixed,invsqrt,cosine,linear}]\n",
            "                  [--lr-scheduler-patience LR_SCHEDULER_PATIENCE]\n",
            "                  [--lr-scheduler-decay LR_SCHEDULER_DECAY]\n",
            "                  [--max-lr-steps MAX_LR_STEPS]\n",
            "                  [--invsqrt-lr-decay-gamma INVSQRT_LR_DECAY_GAMMA]\n",
            "\n",
            "Train a model\n",
            "\n",
            "optional arguments:\n",
            "  -h, --help\n",
            "      show this help message and exit\n",
            "\n",
            "Main ParlAI Arguments:\n",
            "  -o, --init-opt INIT_OPT\n",
            "      Path to json file of options. Note: Further Command-line arguments\n",
            "      override file-based options. (default: None)\n",
            "  -v, --show-advanced-args\n",
            "      Show hidden command line options (advanced users only) (default: False)\n",
            "  -t, --task TASK\n",
            "      ParlAI task(s), e.g. \"babi:Task1\" or \"babi,cbt\" (default: None)\n",
            "  -dt, --datatype {train,train:stream,train:ordered,train:ordered:stream,train:stream:ordered,train:evalmode,train:evalmode:stream,train:evalmode:ordered,train:evalmode:ordered:stream,train:evalmode:stream:ordered,valid,valid:stream,test,test:stream}\n",
            "      choose from: train, train:ordered, valid, test. to stream data add\n",
            "      \":stream\" to any option (e.g., train:stream). by default: train is random\n",
            "      with replacement, valid is ordered, test is ordered. (default: train)\n",
            "  -nt, --numthreads NUMTHREADS\n",
            "      number of threads. Used for hogwild if batchsize is 1, else for number of\n",
            "      threads in threadpool loading, (default: 1)\n",
            "  -bs, --batchsize BATCHSIZE\n",
            "      batch size for minibatch training schemes (default: 1)\n",
            "  -dynb, --dynamic-batching {None,batchsort,full}\n",
            "      Use dynamic batching (default: None)\n",
            "  -dp, --datapath DATAPATH\n",
            "      path to datasets, defaults to {parlai_dir}/data (default: None)\n",
            "\n",
            "ParlAI Model Arguments:\n",
            "  -m, --model MODEL\n",
            "      the model class name. can match parlai/agents/<model> for agents in that\n",
            "      directory, or can provide a fully specified module for `from X import Y`\n",
            "      via `-m X:Y` (e.g. `-m parlai.agents.seq2seq.seq2seq:Seq2SeqAgent`)\n",
            "      (default: None)\n",
            "  -mf, --model-file MODEL_FILE\n",
            "      model file name for loading and saving models (default: None)\n",
            "  -im, --init-model INIT_MODEL\n",
            "      load model weights and dict from this file (default: None)\n",
            "\n",
            "Training Loop Arguments:\n",
            "  -et, --evaltask EVALTASK\n",
            "      task to use for valid/test (defaults to the one used for training)\n",
            "      (default: None)\n",
            "  -eps, --num-epochs NUM_EPOCHS\n",
            "  -ttim, --max-train-time MAX_TRAIN_TIME\n",
            "  -vtim, --validation-every-n-secs VALIDATION_EVERY_N_SECS\n",
            "      Validate every n seconds. Saves model to model_file (if set) whenever best\n",
            "      val metric is found (default: -1)\n",
            "  -stim, --save-every-n-secs SAVE_EVERY_N_SECS\n",
            "      Saves the model to model_file.checkpoint after every n seconds (default\n",
            "      -1, never). (default: -1)\n",
            "  -sval, --save-after-valid SAVE_AFTER_VALID\n",
            "      Saves the model to model_file.checkpoint after every validation (default\n",
            "      False).\n",
            "  -veps, --validation-every-n-epochs VALIDATION_EVERY_N_EPOCHS\n",
            "      Validate every n epochs. Saves model to model_file (if set) whenever best\n",
            "      val metric is found (default: -1)\n",
            "  -vp, --validation-patience VALIDATION_PATIENCE\n",
            "      number of iterations of validation where result does not improve before we\n",
            "      stop training (default: 10)\n",
            "  -vmt, --validation-metric VALIDATION_METRIC\n",
            "      key into report table for selecting best validation (default: accuracy)\n",
            "  -vmm, --validation-metric-mode {max,min}\n",
            "      how to optimize validation metric (max or min) (default: None)\n",
            "  -mcs, --metrics METRICS\n",
            "      list of metrics to show/compute, e.g. all, default,or give a list split by\n",
            "      , like ppl,f1,accuracy,hits@1,rouge,bleuthe rouge metrics will be computed\n",
            "      as rouge-1, rouge-2 and rouge-l (default: default)\n",
            "  -micro, --aggregate-micro AGGREGATE_MICRO\n",
            "      Report micro-averaged metrics instead of macro averaged metrics. (default:\n",
            "      False)\n",
            "\n",
            "Tensorboard Arguments:\n",
            "  -tblog, --tensorboard-log TENSORBOARD_LOG\n",
            "      Tensorboard logging of metrics, default is False\n",
            "\n",
            "Seq2Seq Arguments:\n",
            "  -hs, --hiddensize HIDDENSIZE\n",
            "      size of the hidden layers (default: 128)\n",
            "  -esz, --embeddingsize EMBEDDINGSIZE\n",
            "      size of the token embeddings (default: 128)\n",
            "  -nl, --numlayers NUMLAYERS\n",
            "      number of hidden layers (default: 2)\n",
            "  -dr, --dropout DROPOUT\n",
            "      dropout rate (default: 0.1)\n",
            "  -bi, --bidirectional BIDIRECTIONAL\n",
            "      whether to encode the context with a bidirectional rnn (default: False)\n",
            "  -att, --attention {none,concat,general,dot,local}\n",
            "      Choices: none, concat, general, local. If set local, also set attention-\n",
            "      length. (see arxiv.org/abs/1508.04025) (default: none)\n",
            "  -attl, --attention-length ATTENTION_LENGTH\n",
            "      Length of local attention. (default: 48)\n",
            "  --attention-time {pre,post}\n",
            "      Whether to apply attention before or after decoding. (default: post)\n",
            "  -rnn, --rnn-class {rnn,gru,lstm}\n",
            "      Choose between different types of RNNs. (default: lstm)\n",
            "  -dec, --decoder {same,shared}\n",
            "      Choose between different decoder modules. Default \"same\" uses same class\n",
            "      as encoder, while \"shared\" also uses the same weights. Note that shared\n",
            "      disabled some encoder options--in particular, bidirectionality. (default:\n",
            "      same)\n",
            "  -lt, --lookuptable {unique,enc_dec,dec_out,all}\n",
            "      The encoder, decoder, and output modules can share weights, or not. Unique\n",
            "      has independent embeddings for each. Enc_dec shares the embedding for the\n",
            "      encoder and decoder. Dec_out shares decoder embedding and output weights.\n",
            "      All shares all three weights. (default: unique)\n",
            "  -soft, --numsoftmax NUMSOFTMAX\n",
            "      default 1, if greater then uses mixture of softmax (see\n",
            "      arxiv.org/abs/1711.03953). (default: 1)\n",
            "  -idr, --input-dropout INPUT_DROPOUT\n",
            "      Probability of replacing tokens with UNK in training. (default: 0.0)\n",
            "\n",
            "Torch Generator Agent:\n",
            "  --beam-size BEAM_SIZE\n",
            "      Beam size, if 1 then greedy search (default: 1)\n",
            "  --beam-min-length BEAM_MIN_LENGTH\n",
            "      Minimum length of prediction to be generated by the beam search (default:\n",
            "      1)\n",
            "  --beam-context-block-ngram BEAM_CONTEXT_BLOCK_NGRAM\n",
            "      Size n-grams to block in beam search from the context. val <= 0 implies no\n",
            "      blocking (default: -1)\n",
            "  --beam-block-ngram BEAM_BLOCK_NGRAM\n",
            "      Size n-grams to block in beam search. val <= 0 implies no blocking\n",
            "      (default: -1)\n",
            "  --beam-length-penalty BEAM_LENGTH_PENALTY\n",
            "      Applies a length penalty. Set to 0 for no penalty. (default: 0.65)\n",
            "  --inference {topk,beam,nucleus,delayedbeam,greedy}\n",
            "      Generation algorithm (default: greedy)\n",
            "  --topk TOPK\n",
            "      K used in Top K sampling (default: 10)\n",
            "  --topp TOPP\n",
            "      p used in nucleus sampling (default: 0.9)\n",
            "  --beam-delay BEAM_DELAY\n",
            "      used in delayedbeam search (default: 30)\n",
            "  --temperature TEMPERATURE\n",
            "      temperature to add during decoding (default: 1.0)\n",
            "  --compute-tokenized-bleu COMPUTE_TOKENIZED_BLEU\n",
            "      if true, compute tokenized bleu scores (default: False)\n",
            "\n",
            "TorchAgent Arguments:\n",
            "  -i, --interactive-mode INTERACTIVE_MODE\n",
            "      Whether in full interactive mode or not, which means generating text or\n",
            "      retrieving from a full set of candidates, which is necessary to actually\n",
            "      do full dialogue. However, during training or quick validation (e.g. PPL\n",
            "      for generation or ranking a few candidates for ranking models) you might\n",
            "      want these set to off. Typically, scripts can set their preferred default\n",
            "      behavior at the start, e.g. eval scripts. (default: False)\n",
            "  -emb, --embedding-type {random,glove,glove-fixed,fasttext,fasttext-fixed,fasttext_cc,fasttext_cc-fixed}\n",
            "      Choose between different strategies for initializing word embeddings.\n",
            "      Default is random, but can also preinitialize from Glove or Fasttext.\n",
            "      Preinitialized embeddings can also be fixed so they are not updated during\n",
            "      training. (default: random)\n",
            "  -embp, --embedding-projection EMBEDDING_PROJECTION\n",
            "      If pretrained embeddings have a different dimensionality than your\n",
            "      embedding size, strategy for projecting to the correct size. If the\n",
            "      dimensions are the same, this is ignored unless you append \"-force\" to\n",
            "      your choice. (default: random)\n",
            "  --fp16 FP16\n",
            "      Use fp16 computations. (default: False)\n",
            "  --fp16-impl {apex,mem_efficient}\n",
            "      Implementation of FP16 to use (default: apex)\n",
            "  -rc, --rank-candidates RANK_CANDIDATES\n",
            "      Whether the model should parse candidates for ranking. (default: False)\n",
            "  -tr, --truncate TRUNCATE\n",
            "      Truncate input lengths to increase speed / use less memory. (default: -1)\n",
            "  --text-truncate TEXT_TRUNCATE\n",
            "      Text input truncation length: if not specified, this will default to\n",
            "      `truncate` (default: None)\n",
            "  --label-truncate LABEL_TRUNCATE\n",
            "      Label truncation length: if not specified, this will default to `truncate`\n",
            "      (default: None)\n",
            "  -histsz, --history-size HISTORY_SIZE\n",
            "      Number of past dialog utterances to remember. (default: -1)\n",
            "  -pt, --person-tokens PERSON_TOKENS\n",
            "      add person tokens to history. adds __p1__ in front of input text and\n",
            "      __p2__ in front of past labels when available or past utterances generated\n",
            "      by the model. these are added to the dictionary during initialization.\n",
            "      (default: False)\n",
            "  --split-lines SPLIT_LINES\n",
            "      split the dialogue history on newlines and save in separate vectors\n",
            "      (default: False)\n",
            "  --delimiter DELIMITER\n",
            "      Join history lines with this token, defaults to newline (default: )\n",
            "  -gpu, --gpu GPU\n",
            "      which GPU to use (default: -1)\n",
            "  --no-cuda\n",
            "      disable GPUs even if available. otherwise, will use GPUs if available on\n",
            "      the device. (default: False)\n",
            "\n",
            "Optimizer Arguments:\n",
            "  -opt, --optimizer {adadelta,adagrad,adam,adamw,sparseadam,adamax,asgd,sgd,rprop,rmsprop,optimizer,lbfgs,mem_eff_adam,adafactor}\n",
            "      Choose between pytorch optimizers. Any member of torch.optim should be\n",
            "      valid. (default: sgd)\n",
            "  -lr, --learningrate LEARNINGRATE\n",
            "      Learning rate (default: 1)\n",
            "  -clip, --gradient-clip GRADIENT_CLIP\n",
            "      gradient clipping using l2 norm (default: 0.1)\n",
            "  --adafactor-eps ADAFACTOR_EPS\n",
            "      Epsilon values for adafactor optimizer: regularization constants for\n",
            "      square gradient and parameter scale respectively (default: 1e-30,1e-3)\n",
            "  -mom, --momentum MOMENTUM\n",
            "      if applicable, momentum value for optimizer. (default: 0)\n",
            "  --nesterov NESTEROV\n",
            "      if applicable, whether to use nesterov momentum. (default: True)\n",
            "  -nu, --nus NUS\n",
            "      if applicable, nu value(s) for optimizer. can use a single value like 0.7\n",
            "      or a comma-separated tuple like 0.7,1.0 (default: 0.7)\n",
            "  -beta, --betas BETAS\n",
            "      if applicable, beta value(s) for optimizer. can use a single value like\n",
            "      0.9 or a comma-separated tuple like 0.9,0.999 (default: 0.9,0.999)\n",
            "  -wdecay, --weight-decay WEIGHT_DECAY\n",
            "      Weight decay on the weights. (default: None)\n",
            "\n",
            "BPEHelper Arguments:\n",
            "  --bpe-vocab BPE_VOCAB\n",
            "      path to pre-trained tokenizer vocab (default: None)\n",
            "  --bpe-merge BPE_MERGE\n",
            "      path to pre-trained tokenizer merge (default: None)\n",
            "\n",
            "Learning Rate Scheduler:\n",
            "  --lr-scheduler {reduceonplateau,none,fixed,invsqrt,cosine,linear}\n",
            "      Learning rate scheduler. (default: reduceonplateau)\n",
            "  --lr-scheduler-patience LR_SCHEDULER_PATIENCE\n",
            "      LR scheduler patience. In number of validation runs. If using fixed\n",
            "      scheduler, LR is decayed every <patience> validations. (default: 3)\n",
            "  --lr-scheduler-decay LR_SCHEDULER_DECAY\n",
            "      Decay factor for LR scheduler, or how much LR is multiplied by when it is\n",
            "      lowered. (default: 0.5)\n",
            "  --max-lr-steps MAX_LR_STEPS\n",
            "      Number of train steps the scheduler should take after warmup. Training is\n",
            "      terminated after this many steps. This should only be set for --lr-\n",
            "      scheduler cosine or linear (default: -1)\n",
            "  --invsqrt-lr-decay-gamma INVSQRT_LR_DECAY_GAMMA\n",
            "      Constant used only to find the lr multiplier for the invsqrt scheduler.\n",
            "      Must be set for --lr-scheduler invsqrt (default: -1)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EKGUWyKTwVtX"
      },
      "source": [
        "You'll notice the options are give as commandline arguments. We control our options via `argparse`. The option names are relatively predictable: `--init-model` becomes `init_model`; `--num-epochs` becomes `num_epochs` and so on."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jgLwGAq1wZJb"
      },
      "source": [
        "# Looking at model predictions\n",
        "\n",
        "We have shown how we can chat with a model ourselves, interactively. We might want to inspect how the model reacts with a fixed set of inputs. Let's use that model we just trained!\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UCZgs6OlvJ-q",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        },
        "outputId": "95b89fa8-22a6-4261-d72e-e82b07517810"
      },
      "source": [
        "from parlai.scripts.display_model import DisplayModel\n",
        "DisplayModel.main(\n",
        "    task='empathetic_dialogues',\n",
        "    model_file='from_pretrained/model',\n",
        "    num_examples=2,\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[ warning: overriding opt['num_examples'] to 2 (previously: None )]\n",
            "[ Using CUDA ]\n",
            "Dictionary: loading dictionary from from_pretrained/model.dict\n",
            "[ num words =  54944 ]\n",
            "Total parameters: 87,508,992 (87,508,992 trainable)\n",
            "[ Loading existing model params from from_pretrained/model ]\n",
            "[creating task(s): empathetic_dialogues]\n",
            "[EmpatheticDialoguesTeacher] Only use experiencer side? True, datatype: valid\n",
            "\u001b[1;31m- - - NEW EPISODE: empathetic_dialogues- - -\u001b[0;0m\n",
            "\u001b[0mToday,as i was leaving for work in the morning,i had a tire burst in the middle of a busy road. That scared the hell out of me!\u001b[0;0m\n",
            "\u001b[1;94m    labels: Are you fine now?\u001b[0;0m\n",
            "\u001b[0;95m     model: No response\u001b[0;0m\n",
            "\u001b[0mYeah,i'm doing alright now, but with minor injuries.\u001b[0;0m\n",
            "\u001b[1;94m    labels: Cool :) Is your car damaged a lot?\u001b[0;0m\n",
            "\u001b[0;95m     model: No response\u001b[0;0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2H3QKTjdwokh"
      },
      "source": [
        "Whoa wait a second! The model isn't giving any responses? That's because we set `--skip-generation true` to speed up training. We need to turn that back off."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DLiq-vuowamh",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        },
        "outputId": "241342eb-6ac6-4a0c-91c5-674ab6b7e0c7"
      },
      "source": [
        "from parlai.scripts.display_model import DisplayModel\n",
        "DisplayModel.main(\n",
        "    task='empathetic_dialogues',\n",
        "    model_file='from_pretrained/model',\n",
        "    num_examples=2,\n",
        "    skip_generation=False,\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[ warning: overriding opt['num_examples'] to 2 (previously: None )]\n",
            "[ warning: overriding opt['skip_generation'] to False (previously: True )]\n",
            "[ Using CUDA ]\n",
            "Dictionary: loading dictionary from from_pretrained/model.dict\n",
            "[ num words =  54944 ]\n",
            "Total parameters: 87,508,992 (87,508,992 trainable)\n",
            "[ Loading existing model params from from_pretrained/model ]\n",
            "[creating task(s): empathetic_dialogues]\n",
            "[EmpatheticDialoguesTeacher] Only use experiencer side? True, datatype: valid\n",
            "\u001b[1;31m- - - NEW EPISODE: empathetic_dialogues- - -\u001b[0;0m\n",
            "\u001b[0mToday,as i was leaving for work in the morning,i had a tire burst in the middle of a busy road. That scared the hell out of me!\u001b[0;0m\n",
            "\u001b[1;94m    labels: Are you fine now?\u001b[0;0m\n",
            "\u001b[0;95m     model: oh no ! that ' s terrible ! did you get a new tire ?\u001b[0;0m\n",
            "\u001b[0mYeah,i'm doing alright now, but with minor injuries.\u001b[0;0m\n",
            "\u001b[1;94m    labels: Cool :) Is your car damaged a lot?\u001b[0;0m\n",
            "\u001b[0;95m     model: that ' s good . i hope you are okay .\u001b[0;0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-MR0rn0ZwyxQ"
      },
      "source": [
        "On the command line:\n",
        "```bash\n",
        "python -m parlai.scripts.display_model --task empathetic_dialogues --model-file zoo:tutorial_transformer_generator/model\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jYuaSPWrw0Il"
      },
      "source": [
        "# Bringing your own datasets\n",
        "\n",
        "What if you want to build your own dataset in ParlAI? Of course you can do that!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5SgJi8XHwtph",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        },
        "outputId": "9656b678-9dfc-4cfa-d1eb-3cb13aa14608"
      },
      "source": [
        "from parlai.core.teachers import register_teacher, DialogTeacher\n",
        "\n",
        "@register_teacher(\"my_teacher\")\n",
        "class MyTeacher(DialogTeacher):\n",
        "    def __init__(self, opt, shared=None):\n",
        "        # opt is the command line arguments.\n",
        "        \n",
        "        # What is this shared thing?\n",
        "        # We make many copies of a teacher, one-per-batchsize. Shared lets us store \n",
        "        \n",
        "        # We just need to set the \"datafile\".  This is boilerplate, but differs in many teachers.\n",
        "        # The \"datafile\" is the filename where we will load the data from. In this case, we'll set it to\n",
        "        # the fold name (train/valid/test) + \".txt\"\n",
        "        opt['datafile'] = opt['datatype'].split(':')[0] + \".txt\"\n",
        "        super().__init__(opt, shared)\n",
        "    \n",
        "    def setup_data(self, datafile):\n",
        "        # filename tells us where to load from.\n",
        "        # We'll just use some hardcoded data, but show how you could read the filename here:\n",
        "        print(f\" ~~ Loading from {datafile} ~~ \")\n",
        "        \n",
        "        # setup_data should yield tuples of ((text, label), new_episode)\n",
        "        # That is ((str, str), bool)\n",
        "        \n",
        "        # first episode\n",
        "        # notice how we have call, response, and then True? The True indicates this is a first message\n",
        "        # in a conversation\n",
        "        yield ('Hello', 'Hi'), True\n",
        "        # Next we have the second turn. This time, the last element is False, indicating we're still going\n",
        "        yield ('How are you', 'I am fine'), False\n",
        "        yield (\"Let's say goodbye\", 'Goodbye!'), False\n",
        "        \n",
        "        # second episode. We need to have True again!\n",
        "        yield (\"Hey\", \"hi there\"), True\n",
        "        yield (\"Deja vu?\", \"Deja vu!\"), False\n",
        "        yield (\"Last chance\", \"This is it\"), False\n",
        "        \n",
        "        \n",
        "DisplayData.main(task=\"my_teacher\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[creating task(s): my_teacher]\n",
            " ~~ Loading from train.txt ~~ \n",
            "\u001b[1;31m- - - NEW EPISODE: my_teacher - - -\u001b[0;0m\n",
            "\u001b[0mHello\u001b[0;0m\n",
            "   \u001b[1;94mHi\u001b[0;0m\n",
            "\u001b[0mHow are you\u001b[0;0m\n",
            "   \u001b[1;94mI am fine\u001b[0;0m\n",
            "\u001b[0mLet's say goodbye\u001b[0;0m\n",
            "   \u001b[1;94mGoodbye!\u001b[0;0m\n",
            "\u001b[1;31m- - - NEW EPISODE: my_teacher - - -\u001b[0;0m\n",
            "\u001b[0mHey\u001b[0;0m\n",
            "   \u001b[1;94mhi there\u001b[0;0m\n",
            "\u001b[0mDeja vu?\u001b[0;0m\n",
            "   \u001b[1;94mDeja vu!\u001b[0;0m\n",
            "\u001b[0mLast chance\u001b[0;0m\n",
            "   \u001b[1;94mThis is it\u001b[0;0m\n",
            "EPOCH DONE\n",
            "[ loaded 2 episodes with a total of 6 examples ]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vvwxi6gXw8jU"
      },
      "source": [
        "Notice how the data corresponds to the utterances we provided? In reality, we'd normally want to load up a data file, loop through it, and yield the tuples from processed data. But for this simple example, it works well.\n",
        "\n",
        "We can now use our teacher in the standard places! Let's see how the model we trained earlier behaves with it:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZIyZQnxAw5HG",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 527
        },
        "outputId": "7240d04f-f9d3-4188-9bc8-c67f99b873b5"
      },
      "source": [
        "DisplayModel.main(task='my_teacher', model_file='from_pretrained/model', skip_generation=False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[ warning: overriding opt['task'] to my_teacher (previously: empathetic_dialogues )]\n",
            "[ warning: overriding opt['skip_generation'] to False (previously: True )]\n",
            "[ Using CUDA ]\n",
            "Dictionary: loading dictionary from from_pretrained/model.dict\n",
            "[ num words =  54944 ]\n",
            "Total parameters: 87,508,992 (87,508,992 trainable)\n",
            "[ Loading existing model params from from_pretrained/model ]\n",
            "[creating task(s): my_teacher]\n",
            " ~~ Loading from valid.txt ~~ \n",
            "\u001b[1;31m- - - NEW EPISODE: my_teacher- - -\u001b[0;0m\n",
            "\u001b[0mHello\u001b[0;0m\n",
            "\u001b[1;94m    labels: Hi\u001b[0;0m\n",
            "\u001b[0;95m     model: hi\u001b[0;0m\n",
            "\u001b[0mHow are you\u001b[0;0m\n",
            "\u001b[1;94m    labels: I am fine\u001b[0;0m\n",
            "\u001b[0;95m     model: i am good , how are you ?\u001b[0;0m\n",
            "\u001b[0mLet's say goodbye\u001b[0;0m\n",
            "\u001b[1;94m    labels: Goodbye!\u001b[0;0m\n",
            "\u001b[0;95m     model: i am fine\u001b[0;0m\n",
            "\u001b[1;31m- - - NEW EPISODE: my_teacher- - -\u001b[0;0m\n",
            "\u001b[0mHey\u001b[0;0m\n",
            "\u001b[1;94m    labels: hi there\u001b[0;0m\n",
            "\u001b[0;95m     model: hi\u001b[0;0m\n",
            "\u001b[0mDeja vu?\u001b[0;0m\n",
            "\u001b[1;94m    labels: Deja vu!\u001b[0;0m\n",
            "\u001b[0;95m     model: i ' ve just been in this place before\u001b[0;0m\n",
            "\u001b[0mLast chance\u001b[0;0m\n",
            "\u001b[1;94m    labels: This is it\u001b[0;0m\n",
            "\u001b[0;95m     model: i ' ve just been in this place before\u001b[0;0m\n",
            "EPOCH DONE\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EOzvSHAy0meK"
      },
      "source": [
        "Note that the `register_teacher` decorator makes the commands aware of your teacher. If you leave it off, the commands won't be able to locate it. If you want to use your teacher on the command line, you'll need to put it in a very specific filename: `parlai/agents/my_teacher/agents.py`, and you'll need to name the class `DefaultTeacher` instead of `MyTeacher`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nJj7Lhs00oOB"
      },
      "source": [
        "# Creating your own models\n",
        "\n",
        "As a start, we'll implement a *very* simple agent. This agent will just sort of respond with \"hello X, my name is Y\", where X is based on the input"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pykhtFDrxCPo"
      },
      "source": [
        "from parlai.core.agents import register_agent, Agent\n",
        "\n",
        "@register_agent(\"hello\")\n",
        "class HelloAgent(Agent):\n",
        "    @classmethod\n",
        "    def add_cmdline_args(cls, parser):\n",
        "        parser.add_argument('--name', type=str, default='Alice', help=\"The agent's name.\")\n",
        "        return parser\n",
        "        \n",
        "    def __init__(self, opt, shared=None):\n",
        "        # similar to the teacher, we have the Opt and the shared memory objects!\n",
        "        super().__init__(opt, shared)\n",
        "        self.id = 'HelloAgent'\n",
        "        self.name = opt['name']\n",
        "    \n",
        "    def observe(self, observation):\n",
        "        # Gather the last word from the other user's input\n",
        "        words = observation.get('text', '').split()\n",
        "        if words:\n",
        "            self.last_word = words[-1]\n",
        "        else:\n",
        "            self.last_word = \"stranger!\"\n",
        "    \n",
        "    def act(self):\n",
        "        # Always return a string like this.\n",
        "        return {\n",
        "            'id': self.id,\n",
        "            'text': f\"Hello {self.last_word}, I'm {self.name}\",\n",
        "        }"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x1SZmy_s0sGd"
      },
      "source": [
        "Let's try seeing how this agent behaves:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HcS1UIFH0pb6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "outputId": "bcc52fbf-49f8-47ed-e5db-09e5b6c57b54"
      },
      "source": [
        "DisplayModel.main(task='my_teacher', model='hello')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[creating task(s): my_teacher]\n",
            " ~~ Loading from valid.txt ~~ \n",
            "\u001b[1;31m- - - NEW EPISODE: my_teacher- - -\u001b[0;0m\n",
            "\u001b[0mHello\u001b[0;0m\n",
            "\u001b[1;94m    labels: Hi\u001b[0;0m\n",
            "\u001b[0;95m     model: Hello Hello, I'm Alice\u001b[0;0m\n",
            "\u001b[0mHow are you\u001b[0;0m\n",
            "\u001b[1;94m    labels: I am fine\u001b[0;0m\n",
            "\u001b[0;95m     model: Hello you, I'm Alice\u001b[0;0m\n",
            "\u001b[0mLet's say goodbye\u001b[0;0m\n",
            "\u001b[1;94m    labels: Goodbye!\u001b[0;0m\n",
            "\u001b[0;95m     model: Hello goodbye, I'm Alice\u001b[0;0m\n",
            "\u001b[1;31m- - - NEW EPISODE: my_teacher- - -\u001b[0;0m\n",
            "\u001b[0mHey\u001b[0;0m\n",
            "\u001b[1;94m    labels: hi there\u001b[0;0m\n",
            "\u001b[0;95m     model: Hello Hey, I'm Alice\u001b[0;0m\n",
            "\u001b[0mDeja vu?\u001b[0;0m\n",
            "\u001b[1;94m    labels: Deja vu!\u001b[0;0m\n",
            "\u001b[0;95m     model: Hello vu?, I'm Alice\u001b[0;0m\n",
            "\u001b[0mLast chance\u001b[0;0m\n",
            "\u001b[1;94m    labels: This is it\u001b[0;0m\n",
            "\u001b[0;95m     model: Hello chance, I'm Alice\u001b[0;0m\n",
            "EPOCH DONE\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PmvcRSGS0wQE"
      },
      "source": [
        "Notice how it read the words from the user, and provides its name from the command line argument? We can also interact with it easily enough."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_xd5CaG00tv6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 680
        },
        "outputId": "4d9aee8c-d390-46c7-e685-febcd5a5b91c"
      },
      "source": [
        "Interactive.main(model='hello', name='Bob')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[ optional arguments: ] \n",
            "[  display_examples: False ]\n",
            "[  display_ignore_fields: label_candidates,text_candidates ]\n",
            "[  display_prettify: False ]\n",
            "[  interactive_task: True ]\n",
            "[  name: Bob ]\n",
            "[ Main ParlAI Arguments: ] \n",
            "[  batchsize: 1 ]\n",
            "[  datapath: /usr/local/lib/python3.6/dist-packages/data ]\n",
            "[  datatype: train ]\n",
            "[  download_path: /usr/local/lib/python3.6/dist-packages/downloads ]\n",
            "[  dynamic_batching: None ]\n",
            "[  hide_labels: False ]\n",
            "[  image_mode: raw ]\n",
            "[  init_opt: None ]\n",
            "[  multitask_weights: [1] ]\n",
            "[  numthreads: 1 ]\n",
            "[  show_advanced_args: False ]\n",
            "[  task: interactive ]\n",
            "[ ParlAI Model Arguments: ] \n",
            "[  dict_class: None ]\n",
            "[  init_model: None ]\n",
            "[  model: hello ]\n",
            "[  model_file: None ]\n",
            "[ Local Human Arguments: ] \n",
            "[  local_human_candidates_file: None ]\n",
            "[  single_turn: False ]\n",
            "[ ParlAI Image Preprocessing Arguments: ] \n",
            "[  image_cropsize: 224 ]\n",
            "[  image_size: 256 ]\n",
            "\u001b[1;31mEnter [DONE] if you want to end the episode, [EXIT] to quit.\u001b[0;0m\n",
            "[creating task(s): interactive]\n",
            "\u001b[0mEnter Your Message:\u001b[0;0m hi, who are you?\n",
            "\u001b[0;34m[HelloAgent]:\u001b[0;0m \u001b[1mHello you?, I'm Bob\u001b[0;0m\n",
            "\u001b[0mEnter Your Message:\u001b[0;0m My name is Stephen\n",
            "\u001b[0;34m[HelloAgent]:\u001b[0;0m \u001b[1mHello Stephen, I'm Bob\u001b[0;0m\n",
            "\u001b[0mEnter Your Message:\u001b[0;0m [EXIT]\n",
            "\u001b[0;34m[HelloAgent]:\u001b[0;0m \u001b[1mHello stranger!, I'm Bob\u001b[0;0m\n",
            "CHAT DONE \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qAe1hytf1BPk"
      },
      "source": [
        "Similar to the teacher, the call to `register_agent` makes it available for use in commands. If you forget the `register_agent` decorator, you won't be able to refer to it. Similarly, if you wanted to use this model from the command line, you would need to save this code to a special folder: `parlai/agents/hello/hello.py`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8aBbhKTO1DEE"
      },
      "source": [
        "## Creating a neural network model\n",
        "\n",
        "The base Agent class is very simple, but it also provides extremely little functionality. We have created solid abstractions for creating neural-network type models. [`TorchGeneratorAgent`](https://parl.ai/docs/torch_agent.html#module-parlai.core.torch_generator_agent) is one our common abstractions, and it assumes a model which outputs one-word-at-a-time.\n",
        "\n",
        "The following is from our [ExampleSeq2Seq](https://github.com/facebookresearch/ParlAI/blob/master/parlai/agents/examples/seq2seq.py) agent. It's a simple RNN model, trained like a Machine Translation model. The Model is too complex to go over in this document, but please feel free to [read our TorchGeneratorAgent tutorial](https://parl.ai/docs/tutorial_torch_generator_agent.html)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hVrZh-T903wh"
      },
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import parlai.core.torch_generator_agent as tga\n",
        "\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    \"\"\"\n",
        "    Example encoder, consisting of an embedding layer and a 1-layer LSTM with the\n",
        "    specified hidden size.\n",
        "    Pay particular attention to the ``forward`` output.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, embeddings, hidden_size):\n",
        "        \"\"\"\n",
        "        Initialization.\n",
        "        Arguments here can be used to provide hyperparameters.\n",
        "        \"\"\"\n",
        "        # must call super on all nn.Modules.\n",
        "        super().__init__()\n",
        "\n",
        "        self.embeddings = embeddings\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size=hidden_size,\n",
        "            hidden_size=hidden_size,\n",
        "            num_layers=1,\n",
        "            batch_first=True,\n",
        "        )\n",
        "\n",
        "    def forward(self, input_tokens):\n",
        "        \"\"\"\n",
        "        Perform the forward pass for the encoder.\n",
        "        Input *must* be input_tokens, which are the context tokens given\n",
        "        as a matrix of lookup IDs.\n",
        "        :param input_tokens:\n",
        "            Input tokens as a bsz x seqlen LongTensor.\n",
        "            Likely will contain padding.\n",
        "        :return:\n",
        "            You can return anything you like; it is will be passed verbatim\n",
        "            into the decoder for conditioning. However, it should be something\n",
        "            you can easily manipulate in ``reorder_encoder_states``.\n",
        "            This particular implementation returns the hidden and cell states from the\n",
        "            LSTM.\n",
        "        \"\"\"\n",
        "        embedded = self.embeddings(input_tokens)\n",
        "        _output, hidden = self.lstm(embedded)\n",
        "        return hidden\n",
        "\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    \"\"\"\n",
        "    Basic example decoder, consisting of an embedding layer and a 1-layer LSTM with the\n",
        "    specified hidden size. Decoder allows for incremental decoding by ingesting the\n",
        "    current incremental state on each forward pass.\n",
        "    Pay particular note to the ``forward``.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, embeddings, hidden_size):\n",
        "        \"\"\"\n",
        "        Initialization.\n",
        "        Arguments here can be used to provide hyperparameters.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.embeddings = embeddings\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size=hidden_size,\n",
        "            hidden_size=hidden_size,\n",
        "            num_layers=1,\n",
        "            batch_first=True,\n",
        "        )\n",
        "\n",
        "    def forward(self, input, encoder_state, incr_state=None):\n",
        "        \"\"\"\n",
        "        Run forward pass.\n",
        "        :param input:\n",
        "            The currently generated tokens from the decoder.\n",
        "        :param encoder_state:\n",
        "            The output from the encoder module.\n",
        "        :parm incr_state:\n",
        "            The previous hidden state of the decoder.\n",
        "        \"\"\"\n",
        "        embedded = self.embeddings(input)\n",
        "        if incr_state is None:\n",
        "            # this is our very first call. We want to seed the LSTM with the\n",
        "            # hidden state of the decoder\n",
        "            state = encoder_state\n",
        "        else:\n",
        "            # We've generated some tokens already, so we can reuse the existing\n",
        "            # decoder state\n",
        "            state = incr_state\n",
        "\n",
        "        # get the new output and decoder incremental state\n",
        "        output, incr_state = self.lstm(embedded, state)\n",
        "\n",
        "        return output, incr_state\n",
        "\n",
        "\n",
        "class ExampleModel(tga.TorchGeneratorModel):\n",
        "    \"\"\"\n",
        "    ExampleModel implements the abstract methods of TorchGeneratorModel to define how to\n",
        "    re-order encoder states and decoder incremental states.\n",
        "    It also instantiates the embedding table, encoder, and decoder, and defines the\n",
        "    final output layer.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, dictionary, hidden_size=1024):\n",
        "        super().__init__(\n",
        "            padding_idx=dictionary[dictionary.null_token],\n",
        "            start_idx=dictionary[dictionary.start_token],\n",
        "            end_idx=dictionary[dictionary.end_token],\n",
        "            unknown_idx=dictionary[dictionary.unk_token],\n",
        "        )\n",
        "        self.embeddings = nn.Embedding(len(dictionary), hidden_size)\n",
        "        self.encoder = Encoder(self.embeddings, hidden_size)\n",
        "        self.decoder = Decoder(self.embeddings, hidden_size)\n",
        "\n",
        "    def output(self, decoder_output):\n",
        "        \"\"\"\n",
        "        Perform the final output -> logits transformation.\n",
        "        \"\"\"\n",
        "        return F.linear(decoder_output, self.embeddings.weight)\n",
        "\n",
        "    def reorder_encoder_states(self, encoder_states, indices):\n",
        "        \"\"\"\n",
        "        Reorder the encoder states to select only the given batch indices.\n",
        "        Since encoder_state can be arbitrary, you must implement this yourself.\n",
        "        Typically you will just want to index select on the batch dimension.\n",
        "        \"\"\"\n",
        "        h, c = encoder_states\n",
        "        return h[:, indices, :], c[:, indices, :]\n",
        "\n",
        "    def reorder_decoder_incremental_state(self, incr_state, indices):\n",
        "        \"\"\"\n",
        "        Reorder the decoder states to select only the given batch indices.\n",
        "        This method can be a stub which always returns None; this will result in the\n",
        "        decoder doing a complete forward pass for every single token, making generation\n",
        "        O(n^2). However, if any state can be cached, then this method should be\n",
        "        implemented to reduce the generation complexity to O(n).\n",
        "        \"\"\"\n",
        "        h, c = incr_state\n",
        "        return h[:, indices, :], c[:, indices, :]\n",
        "\n",
        "\n",
        "@register_agent(\"my_first_lstm\")\n",
        "class Seq2seqAgent(tga.TorchGeneratorAgent):\n",
        "    \"\"\"\n",
        "    Example agent.\n",
        "    Implements the interface for TorchGeneratorAgent. The minimum requirement is that it\n",
        "    implements ``build_model``, but we will want to include additional command line\n",
        "    parameters.\n",
        "    \"\"\"\n",
        "\n",
        "    @classmethod\n",
        "    def add_cmdline_args(cls, argparser):\n",
        "        \"\"\"\n",
        "        Add CLI arguments.\n",
        "        \"\"\"\n",
        "        # Make sure to add all of TorchGeneratorAgent's arguments\n",
        "        super(Seq2seqAgent, cls).add_cmdline_args(argparser)\n",
        "\n",
        "        # Add custom arguments only for this model.\n",
        "        group = argparser.add_argument_group('Example TGA Agent')\n",
        "        group.add_argument(\n",
        "            '-hid', '--hidden-size', type=int, default=1024, help='Hidden size.'\n",
        "        )\n",
        "\n",
        "    def build_model(self):\n",
        "        \"\"\"\n",
        "        Construct the model.\n",
        "        \"\"\"\n",
        "\n",
        "        model = ExampleModel(self.dict, self.opt['hidden_size'])\n",
        "        # Optionally initialize pre-trained embeddings by copying them from another\n",
        "        # source: GloVe, fastText, etc.\n",
        "        self._copy_embeddings(model.embeddings.weight, self.opt['embedding_type'])\n",
        "        return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vfR9w_Hm1HHY"
      },
      "source": [
        "Of course, now we can train with our new model. Let's train it on our toy task that we created earlier."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SJMXpogz1E-_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 952
        },
        "outputId": "3fe8275b-a7cd-491a-a761-fbd5849e2a7c"
      },
      "source": [
        "# of course, we can train the model! Let's Train it on our silly toy task from above\n",
        "!rm -rf my_first_lstm\n",
        "!mkdir -p my_first_lstm\n",
        "\n",
        "TrainModel.main(\n",
        "    model='my_first_lstm',\n",
        "    model_file='my_first_lstm/model',\n",
        "    task='my_teacher',\n",
        "    batchsize=1,\n",
        "    validation_every_n_secs=10,\n",
        "    max_train_time=60,\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Building dictionary: 100%|██████████| 6.00/6.00 [00:00<00:00, 1.91kex/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[ building dictionary first... ]\n",
            "[creating task(s): my_teacher]\n",
            " ~~ Loading from train.txt ~~ \n",
            " ~~ Loading from train.txt ~~ \n",
            "Dictionary: saving dictionary to my_first_lstm/model.dict\n",
            "[ dictionary built with 30 tokens in 0s ]\n",
            "[ no model with opt yet at: my_first_lstm/model(.opt) ]\n",
            "[ Using CUDA ]\n",
            "Dictionary: loading dictionary from my_first_lstm/model.dict\n",
            "[ num words =  30 ]\n",
            "Total parameters: 16,824,320 (16,824,320 trainable)\n",
            "[creating task(s): my_teacher]\n",
            " ~~ Loading from train.txt ~~ \n",
            "[ training... ]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[ time:10.0s total_exs:1828 epochs:304.67 ]\n",
            "     clip  exs  gnorm  gpu_mem   loss  lr   ppl  token_acc  total_train_updates   tpb  updates\n",
            "   .01641 1828  1.368    .9171 .04105   1 1.042      .9942                 1828 3.328     1828\n",
            "\n",
            "[ time:10.0s total_exs:1828 epochs:304.67 ]\n",
            "    gpu_mem  lr  total_train_updates\n",
            "      .9171   1                 1828\n",
            "\n",
            "[creating task(s): my_teacher]\n",
            " ~~ Loading from valid.txt ~~ \n",
            "[ running eval: valid ]\n",
            "[ eval completed in 0.06s ]\n",
            "valid:\n",
            "    accuracy   bleu-4  exs  f1  gpu_mem  loss  lr  ppl  token_acc  total_train_updates   tpb\n",
            "           1 .0003337    6   1    .9171     0   1    1          1                 1828 3.333\n",
            "\n",
            "[ new best accuracy: 1 ]\n",
            "[ saving best valid model: my_first_lstm/model ]\n",
            "[ task solved! stopping. ]\n",
            "[ Using CUDA ]\n",
            "Dictionary: loading dictionary from my_first_lstm/model.dict\n",
            "[ num words =  30 ]\n",
            "Total parameters: 16,824,320 (16,824,320 trainable)\n",
            "[ Loading existing model params from my_first_lstm/model ]\n",
            "[creating task(s): my_teacher]\n",
            " ~~ Loading from valid.txt ~~ \n",
            "[ running eval: valid ]\n",
            "[ eval completed in 0.05s ]\n",
            "valid:\n",
            "    accuracy   bleu-4  exs  f1  gpu_mem  loss  lr  ppl  token_acc  total_train_updates   tpb\n",
            "           1 .0003337    6   1    .9131     0   1    1          1                 1828 3.333\n",
            "\n",
            "[creating task(s): my_teacher]\n",
            " ~~ Loading from test.txt ~~ \n",
            "[ running eval: test ]\n",
            "[ eval completed in 0.05s ]\n",
            "test:\n",
            "    accuracy   bleu-4  exs  f1  gpu_mem  loss  lr  ppl  token_acc  total_train_updates   tpb\n",
            "           1 .0003337    6   1    .9131     0   1    1          1                 1828 3.333\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8hHrruVd1KnK"
      },
      "source": [
        "Let's see how it does. It should reproduce the data perfectly:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "shqFpdrE1Iif",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 493
        },
        "outputId": "1a89571c-2a42-48b8-9752-dffc4a58e557"
      },
      "source": [
        "DisplayModel.main(model_file='my_first_lstm/model', task='my_teacher')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[ Using CUDA ]\n",
            "Dictionary: loading dictionary from my_first_lstm/model.dict\n",
            "[ num words =  30 ]\n",
            "Total parameters: 16,824,320 (16,824,320 trainable)\n",
            "[ Loading existing model params from my_first_lstm/model ]\n",
            "[creating task(s): my_teacher]\n",
            " ~~ Loading from valid.txt ~~ \n",
            "\u001b[1;31m- - - NEW EPISODE: my_teacher- - -\u001b[0;0m\n",
            "\u001b[0mHello\u001b[0;0m\n",
            "\u001b[1;94m    labels: Hi\u001b[0;0m\n",
            "\u001b[0;95m     model: Hi\u001b[0;0m\n",
            "\u001b[0mHow are you\u001b[0;0m\n",
            "\u001b[1;94m    labels: I am fine\u001b[0;0m\n",
            "\u001b[0;95m     model: I am fine\u001b[0;0m\n",
            "\u001b[0mLet's say goodbye\u001b[0;0m\n",
            "\u001b[1;94m    labels: Goodbye!\u001b[0;0m\n",
            "\u001b[0;95m     model: Goodbye !\u001b[0;0m\n",
            "\u001b[1;31m- - - NEW EPISODE: my_teacher- - -\u001b[0;0m\n",
            "\u001b[0mHey\u001b[0;0m\n",
            "\u001b[1;94m    labels: hi there\u001b[0;0m\n",
            "\u001b[0;95m     model: hi there\u001b[0;0m\n",
            "\u001b[0mDeja vu?\u001b[0;0m\n",
            "\u001b[1;94m    labels: Deja vu!\u001b[0;0m\n",
            "\u001b[0;95m     model: Deja vu !\u001b[0;0m\n",
            "\u001b[0mLast chance\u001b[0;0m\n",
            "\u001b[1;94m    labels: This is it\u001b[0;0m\n",
            "\u001b[0;95m     model: This is it\u001b[0;0m\n",
            "EPOCH DONE\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3hzrDhPS1QCW"
      },
      "source": [
        "Unsurprisingly, we got perfect accuracy. This is because the data set is only a handful of utterances, and we can perfectly memorize it in this LSTM. Nonetheless, a great success!\n",
        "\n",
        "# What's next!\n",
        "\n",
        "The sky's the limit! Be sure to check out our [GitHub](https://github.com/facebookresearch/ParlAI) and [Follow ParlAI on Twitter](https://twitter.com/parlai_parley). We're eager to hear what you are using ParlAI for!\n",
        "\n",
        "Here are some other great resources:\n",
        "- [Our research page](https://parl.ai/projects/)\n",
        "- [ParlAI Documentations](https://parl.ai/docs/index.html)\n",
        "- [Tutorial: Writing a Ranker model](https://parl.ai/docs/tutorial_torch_ranker_agent.html)\n",
        "- [Tutorial: Using Mechanical Turk](https://parl.ai/docs/tutorial_mturk.html)\n",
        "- [Tutorial: Connecting to chat services](https://parl.ai/docs/tutorial_chat_service.html)"
      ]
    }
  ]
}